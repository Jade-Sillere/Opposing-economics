{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2fa6e6-c74b-496f-a80c-6da6125693ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To look at trying to take intermediate layer, not simply the last layer of BERT embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91fca1be-808c-4311-af25-3526c29090f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing packages\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import cosine_similarity\n",
    "import random\n",
    "#!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75644ed0-4ede-4a89-957e-4c5f34fc30a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to load the BERT tokenizer + model - sanity check to check models can be loaded\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de289f00-5fc2-427e-a5e6-152b8dd4a3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adam smith an inquiry into the nature and causes of the wealth of nations introduction and plan of the work . the annual labour of every nation is the fund which originally supplies it with all the necessaries and conveniencies of life which it annually consumes , and which consist always either in the immediate produce of that labour , or in what is purchased with that produce from other nations . according , therefore , as this produce , or what is purchased with it , bears a greater or smaller proportion to the number of those who are to consume it , the nation will be better or worse supplied with all the necessaries and conveniencies for which it has occasion . but this proportion must in every nation be regulated by two different circumstances : first , by the skill , dexterity , and judgment with which its labour is generally applied ; and , secondly , by the proportion between the number of those who are employed in useful labour , and that of those who are not so employed . whatever be the soil , climate , or extent of territory of any particular nation , the abundance or scantiness of its annual supply must , in that particular situation , depend upon those two circumstances . the abundance or scantiness of this supply , too , seems to depend more upon the former of those two circumstances than upon the latter . among the savage nations of hunters and fishers , every individual who is able to work is more or less employed in useful labour , and endeavours to provide , as well as he can , the necessaries and conveniencies of life , for himself , and such of his family or tribe as are either too old , or too young , or too infirm , to go a - hunting and fishing . such nations , however , are so miserably poor , that , from mere want , they are frequently reduced , or at least think themselves reduced , to the necessity sometimes of directly destroying , and sometimes of abandoning their infants , their old people , and those afflicted with lingering diseases , to perish with hunger , or to be devoured by wild beasts . among civilized and thriving nations , on the . contrary , though a great number of people do not labour at all , many of whom consume the produce of ten times , frequently of a hundred times , more labour than the greater part of those who work ; yet the produce of the whole labour of the society is so great , that all are often abundantly supplied ; and a workman , even of the\n"
     ]
    }
   ],
   "source": [
    "# Function to chunk the text into smaller parts to meet limit of BERT model of 512 tokens + testing with Wealth of Nations by Adam Smith\n",
    "def chunk_text(text, max_length=500):\n",
    "    # Tokenize without truncation so we keep all tokens\n",
    "    tokens = bert_tokenizer.tokenize(text)\n",
    "\n",
    "    # Break into chunks\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), max_length):\n",
    "        chunk_tokens = tokens[i:i + max_length]\n",
    "        chunk_text = bert_tokenizer.convert_tokens_to_string(chunk_tokens)\n",
    "        chunks.append(chunk_text)\n",
    "    return chunks\n",
    "\n",
    "# Read the text file of Wealth of Nations\n",
    "with open('A_wealth_of_Nations_Cleaned.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Get the chunks\n",
    "chunks = chunk_text(text)\n",
    "\n",
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "300645ab-3d48-4d86-9784-a4e918e63a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##ter , partly by the general circumstances of the society or neighbourhood in which the land is situated , and partly by the natural or improved fertility of the land . these ordinary or average rates may be called the natural rates of wages , profit and rent , at the time and place in which they commonly prevail . when the price of any commodity is neither more nor less than what is sufficient to pay the rent of the land , the wages of the labour , and the profits of the stock employed in raising , preparing , and bringing it to market , according to their natural rates , the commodity is then sold for what may be called its natural price . the commodity is then sold precisely for what it is worth , or for what it really costs the person who brings it to market ; for though , in common language , what is called the prime cost of any commodity does not comprehend the profit of the person who is to sell it again , yet , if he sells it at a price which does not allow him the ordinary rate of profit in his neighbourhood , he is evidently a loser by the trade ; since , by employing his stock in some other way , he might have made that profit . his profit , besides , is his revenue , the proper fund of his subsistence . as , while he is preparing and bringing the goods to market , he advances to his workmen their wages , or their subsistence ; so he advances to himself , in the same manner , his own subsistence , which is generally suitable to the profit which he may reasonably expect from the sale of his goods . unless they yield him this profit , therefore , they do not repay him what they may very properly be said to have really cost him . though the price , therefore , which leaves him this profit , is not always the lowest at which a dealer may sometimes sell his goods , it is the lowest at which he is likely to sell them for any considerable time ; at least where there is perfect liberty , or where he may change his trade as often as he pleases . the actual price at which any commodity is commonly sold , is called its market price . it may either be above , or below , or exactly the same with its natural price . the market price of every particular commodity is regulated by the proportion between the quantity which is actually brought to market , and the demand of those who are willing to pay the natural price of the commodity , or the whole value of the rent , labour , and profit , which must be paid in order to bring\n"
     ]
    }
   ],
   "source": [
    "print(chunks[50]) # sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356e68d9-bab8-4d0d-953e-a89453a95596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#\n",
    "#\n",
    "#\n",
    "#                                                       Descriptive Analysis: TF-IDF\n",
    "#\n",
    "#\n",
    "#\n",
    "# ------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ea1fd24-9e42-4da8-b41f-5291b4af12d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ------ --------------------------------- 2.1/12.8 MB 13.1 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 5.2/12.8 MB 13.3 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 8.1/12.8 MB 13.2 MB/s eta 0:00:01\n",
      "     -------------------------------- ------ 10.7/12.8 MB 13.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.6/12.8 MB 13.1 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 12.0 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# Installing spacy english model\n",
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d22c124d-2db1-4220-bf4e-4c985dbe26ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top TF-IDF Terms in The Wealth of Nations:\n",
      "\n",
      "great           0.4056\n",
      "country         0.2599\n",
      "price           0.1915\n",
      "produce         0.1585\n",
      "trade           0.1579\n",
      "labour          0.1496\n",
      "time            0.1420\n",
      "revenue         0.1418\n",
      "good            0.1414\n",
      "land            0.1273\n",
      "quantity        0.1249\n",
      "different       0.1218\n",
      "value           0.1200\n",
      "pay             0.1177\n",
      "stock           0.1171\n",
      "people          0.1126\n",
      "capital         0.1105\n",
      "money           0.1093\n",
      "profit          0.1060\n",
      "employ          0.1022\n",
      "year            0.0997\n",
      "silver          0.0950\n",
      "expense         0.0891\n",
      "market          0.0889\n",
      "corn            0.0878\n",
      "increase        0.0846\n",
      "tax             0.0842\n",
      "proportion      0.0751\n",
      "particular      0.0737\n",
      "gold            0.0730\n",
      "rent            0.0716\n",
      "order           0.0705\n",
      "foreign         0.0697\n",
      "state           0.0685\n",
      "manufacture     0.0682\n",
      "present         0.0675\n",
      "commodity       0.0674\n",
      "man             0.0665\n",
      "occasion        0.0655\n",
      "annual          0.0646\n",
      "suppose         0.0644\n",
      "colony          0.0644\n",
      "small           0.0639\n",
      "necessary       0.0626\n",
      "duty            0.0626\n",
      "shilling        0.0614\n",
      "pound           0.0608\n",
      "industry        0.0588\n",
      "manner          0.0583\n",
      "europe          0.0582\n",
      "\n",
      " Top TF-IDF Terms in The Communist Manifesto:\n",
      "\n",
      "class           0.3171\n",
      "communist       0.2883\n",
      "bourgeois       0.2551\n",
      "bourgeoisie     0.2303\n",
      "proletariat     0.2237\n",
      "society         0.1933\n",
      "proletarian     0.1756\n",
      "work            0.1627\n",
      "property        0.1568\n",
      "manifesto       0.1508\n",
      "worker          0.1474\n",
      "production      0.1379\n",
      "commune         0.1292\n",
      "industry        0.1167\n",
      "development     0.1160\n",
      "engel           0.1110\n",
      "condition       0.1061\n",
      "time            0.1049\n",
      "state           0.1049\n",
      "revolution      0.1014\n",
      "social          0.0990\n",
      "form            0.0978\n",
      "marx            0.0977\n",
      "country         0.0967\n",
      "communism       0.0878\n",
      "paris           0.0872\n",
      "new             0.0872\n",
      "party           0.0860\n",
      "edition         0.0837\n",
      "draft           0.0779\n",
      "socialism       0.0762\n",
      "german          0.0754\n",
      "socialist       0.0746\n",
      "political       0.0743\n",
      "labor           0.0729\n",
      "mean            0.0707\n",
      "international   0.0696\n",
      "power           0.0695\n",
      "man             0.0672\n",
      "hand            0.0672\n",
      "modern          0.0660\n",
      "exist           0.0660\n",
      "capital         0.0648\n",
      "movement        0.0637\n",
      "private         0.0625\n",
      "free            0.0625\n",
      "relation        0.0625\n",
      "capitalist      0.0613\n",
      "french          0.0613\n",
      "labour          0.0601\n"
     ]
    }
   ],
   "source": [
    "# Performing TF-IDF on the entire text of Wealth of Nations by Adam Smith and Communist Manifesto by Karl Marx\n",
    "#   Aim: to extract intersect of economics terms (among the top 50 TF-IDF of each text)\n",
    "import re\n",
    "import spacy\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.max_length = 5000000 \n",
    "\n",
    "# Reading texts separately\n",
    "with open('A_wealth_of_Nations_Cleaned.txt', 'r', encoding='utf-8') as file:\n",
    "    text1 = file.read()\n",
    "\n",
    "with open('Communist Manifesto.txt', 'r', encoding='utf-8') as file:\n",
    "    text2 = file.read()\n",
    "\n",
    "# Text cleaning + lemmatization function\n",
    "def preprocess(text):\n",
    "    text = text.lower()                                # Convert all characters to lowercase\n",
    "    text = re.sub(r'\\d+', '', text)                    # Remove numbers\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)                # Remove punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()           # Normalize whitespace\n",
    "    \n",
    "    doc = nlp(text)  # Processing the cleaned text using a spaCy NLP pipeline\n",
    "    lemmas = [token.lemma_ for token in doc \n",
    "              if not token.is_stop and not token.is_punct and token.lemma_ != '-PRON-']\n",
    "    return ' '.join(lemmas)\n",
    "\n",
    "# Preprocessing each document\n",
    "cleaned_texts = [preprocess(text1), preprocess(text2)]\n",
    "\n",
    "# Performing TF-IDF vectorization across the two documents\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=1000) # removes stopwords\n",
    "X = vectorizer.fit_transform(cleaned_texts)\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Getting TF-IDF scores for each doc\n",
    "tfidf_scores_doc1 = X.toarray()[0]\n",
    "tfidf_scores_doc2 = X.toarray()[1]\n",
    "\n",
    "# Getting top terms per document\n",
    "def get_top_terms(tfidf_scores, terms, top_n=50):\n",
    "    top_indices = np.argsort(tfidf_scores)[::-1]\n",
    "    return [(terms[i], tfidf_scores[i]) for i in top_indices[:top_n]]\n",
    "\n",
    "top_terms_doc1 = get_top_terms(tfidf_scores_doc1, terms)\n",
    "top_terms_doc2 = get_top_terms(tfidf_scores_doc2, terms)\n",
    "\n",
    "# Displaying results\n",
    "print(\"\\nTop TF-IDF Terms in The Wealth of Nations:\\n\")\n",
    "for word, score in top_terms_doc1:\n",
    "    print(f\"{word:<15} {score:.4f}\")\n",
    "\n",
    "print(\"\\n Top TF-IDF Terms in The Communist Manifesto:\\n\")\n",
    "for word, score in top_terms_doc2:\n",
    "    print(f\"{word:<15} {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe69a048-0511-43a1-9d1e-17e1f5e4b428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top TF-IDF Terms in The Wealth of Nations:\n",
      "\n",
      "great           0.4056\n",
      "country         0.2599\n",
      "price           0.1915\n",
      "produce         0.1585\n",
      "trade           0.1579\n",
      "labour          0.1496\n",
      "time            0.1420\n",
      "revenue         0.1418\n",
      "good            0.1414\n",
      "land            0.1273\n",
      "quantity        0.1249\n",
      "different       0.1218\n",
      "value           0.1200\n",
      "pay             0.1177\n",
      "stock           0.1171\n",
      "people          0.1126\n",
      "capital         0.1105\n",
      "money           0.1093\n",
      "profit          0.1060\n",
      "employ          0.1022\n",
      "year            0.0997\n",
      "silver          0.0950\n",
      "expense         0.0891\n",
      "market          0.0889\n",
      "corn            0.0878\n",
      "increase        0.0846\n",
      "tax             0.0842\n",
      "proportion      0.0751\n",
      "particular      0.0737\n",
      "gold            0.0730\n",
      "rent            0.0716\n",
      "order           0.0705\n",
      "foreign         0.0697\n",
      "state           0.0685\n",
      "manufacture     0.0682\n",
      "present         0.0675\n",
      "commodity       0.0674\n",
      "man             0.0665\n",
      "occasion        0.0655\n",
      "annual          0.0646\n",
      "suppose         0.0644\n",
      "colony          0.0644\n",
      "small           0.0639\n",
      "necessary       0.0626\n",
      "duty            0.0626\n",
      "shilling        0.0614\n",
      "pound           0.0608\n",
      "industry        0.0588\n",
      "manner          0.0583\n",
      "europe          0.0582\n",
      "england         0.0579\n",
      "public          0.0578\n",
      "carry           0.0552\n",
      "purchase        0.0552\n",
      "number          0.0543\n",
      "nation          0.0542\n",
      "frequently      0.0539\n",
      "britain         0.0536\n",
      "work            0.0526\n",
      "government      0.0524\n",
      "high            0.0520\n",
      "exportation     0.0519\n",
      "necessarily     0.0517\n",
      "place           0.0514\n",
      "society         0.0514\n",
      "wage            0.0511\n",
      "account         0.0497\n",
      "employment      0.0496\n",
      "equal           0.0493\n",
      "maintain        0.0493\n",
      "generally       0.0491\n",
      "real            0.0486\n",
      "case            0.0484\n",
      "raise           0.0483\n",
      "improvement     0.0481\n",
      "bank            0.0476\n",
      "law             0.0473\n",
      "merchant        0.0470\n",
      "bounty          0.0464\n",
      "kind            0.0460\n",
      "company         0.0457\n",
      "demand          0.0454\n",
      "afford          0.0453\n",
      "sort            0.0448\n",
      "common          0.0435\n",
      "bring           0.0434\n",
      "taxis           0.0430\n",
      "altogether      0.0428\n",
      "accord          0.0428\n",
      "importation     0.0426\n",
      "person          0.0425\n",
      "home            0.0425\n",
      "little          0.0422\n",
      "mean            0.0418\n",
      "general         0.0418\n",
      "considerable    0.0418\n",
      "likely          0.0414\n",
      "certain         0.0412\n",
      "sell            0.0407\n",
      "say             0.0407\n",
      "consumption     0.0397\n",
      "sovereign       0.0396\n",
      "naturally       0.0392\n",
      "coin            0.0391\n",
      "exchange        0.0386\n",
      "new             0.0386\n",
      "fall            0.0384\n",
      "establish       0.0378\n",
      "rate            0.0376\n",
      "ordinary        0.0376\n",
      "seldom          0.0374\n",
      "rise            0.0369\n",
      "subject         0.0366\n",
      "inhabitant      0.0365\n",
      "commonly        0.0363\n",
      "cent            0.0358\n",
      "come            0.0352\n",
      "thing           0.0351\n",
      "commerce        0.0349\n",
      "long            0.0346\n",
      "scotland        0.0345\n",
      "supply          0.0345\n",
      "render          0.0336\n",
      "natural         0.0336\n",
      "war             0.0335\n",
      "france          0.0335\n",
      "principal       0.0333\n",
      "far             0.0326\n",
      "ancient         0.0325\n",
      "oblige          0.0323\n",
      "low             0.0323\n",
      "effect          0.0322\n",
      "sufficient      0.0319\n",
      "require         0.0319\n",
      "metal           0.0307\n",
      "advantage       0.0305\n",
      "support         0.0300\n",
      "farmer          0.0297\n",
      "export          0.0295\n",
      "probably        0.0295\n",
      "rich            0.0293\n",
      "town            0.0292\n",
      "king            0.0291\n",
      "cheap           0.0289\n",
      "sum             0.0287\n",
      "america         0.0287\n",
      "landlord        0.0286\n",
      "consider        0.0283\n",
      "use             0.0283\n",
      "rude            0.0283\n",
      "like            0.0277\n",
      "private         0.0274\n",
      "master          0.0274\n",
      "monopoly        0.0273\n",
      "fortune         0.0273\n",
      "annually        0.0271\n",
      "weight          0.0269\n",
      "branch          0.0266\n",
      "fund            0.0266\n",
      "way             0.0266\n",
      "diminish        0.0264\n",
      "buy             0.0264\n",
      "poor            0.0264\n",
      "century         0.0263\n",
      "continue        0.0261\n",
      "nearly          0.0260\n",
      "draw            0.0260\n",
      "ought           0.0260\n",
      "extraordinary   0.0258\n",
      "workman         0.0257\n",
      "soon            0.0256\n",
      "contribute      0.0254\n",
      "manufacturer    0.0254\n",
      "appear          0.0253\n",
      "proper          0.0253\n",
      "consequence     0.0253\n",
      "exceed          0.0252\n",
      "free            0.0251\n",
      "productive      0.0249\n",
      "regulation      0.0248\n",
      "augment         0.0248\n",
      "advance         0.0247\n",
      "wealth          0.0246\n",
      "degree          0.0246\n",
      "reduce          0.0246\n",
      "circumstance    0.0246\n",
      "impose          0.0243\n",
      "deal            0.0243\n",
      "debt            0.0237\n",
      "inferior        0.0236\n",
      "cultivation     0.0236\n",
      "labourer        0.0236\n",
      "observe         0.0234\n",
      "thousand        0.0234\n",
      "subsistence     0.0233\n",
      "somewhat        0.0233\n",
      "example         0.0228\n",
      "custom          0.0228\n",
      "power           0.0227\n",
      "food            0.0226\n",
      "\n",
      "Top TF-IDF Terms in The Communist Manifesto:\n",
      "\n",
      "class           0.3171\n",
      "communist       0.2883\n",
      "bourgeois       0.2551\n",
      "bourgeoisie     0.2303\n",
      "proletariat     0.2237\n",
      "society         0.1933\n",
      "proletarian     0.1756\n",
      "work            0.1627\n",
      "property        0.1568\n",
      "manifesto       0.1508\n",
      "worker          0.1474\n",
      "production      0.1379\n",
      "commune         0.1292\n",
      "industry        0.1167\n",
      "development     0.1160\n",
      "engel           0.1110\n",
      "condition       0.1061\n",
      "time            0.1049\n",
      "state           0.1049\n",
      "revolution      0.1014\n",
      "social          0.0990\n",
      "form            0.0978\n",
      "marx            0.0977\n",
      "country         0.0967\n",
      "communism       0.0878\n",
      "paris           0.0872\n",
      "new             0.0872\n",
      "party           0.0860\n",
      "edition         0.0837\n",
      "draft           0.0779\n",
      "socialism       0.0762\n",
      "german          0.0754\n",
      "socialist       0.0746\n",
      "political       0.0743\n",
      "labor           0.0729\n",
      "mean            0.0707\n",
      "international   0.0696\n",
      "power           0.0695\n",
      "man             0.0672\n",
      "hand            0.0672\n",
      "modern          0.0660\n",
      "exist           0.0660\n",
      "capital         0.0648\n",
      "movement        0.0637\n",
      "private         0.0625\n",
      "free            0.0625\n",
      "relation        0.0625\n",
      "capitalist      0.0613\n",
      "french          0.0613\n",
      "labour          0.0601\n",
      "revolutionary   0.0596\n",
      "big             0.0596\n",
      "great           0.0589\n",
      "struggle        0.0566\n",
      "germany         0.0566\n",
      "order           0.0566\n",
      "government      0.0566\n",
      "develop         0.0563\n",
      "way             0.0554\n",
      "industrial      0.0547\n",
      "france          0.0542\n",
      "competition     0.0542\n",
      "old             0.0542\n",
      "rule            0.0530\n",
      "people          0.0519\n",
      "existence       0.0519\n",
      "force           0.0519\n",
      "programme       0.0514\n",
      "national        0.0483\n",
      "league          0.0483\n",
      "demand          0.0472\n",
      "long            0.0472\n",
      "member          0.0472\n",
      "historical      0.0460\n",
      "publish         0.0460\n",
      "question        0.0424\n",
      "feudal          0.0424\n",
      "nation          0.0424\n",
      "common          0.0424\n",
      "english         0.0424\n",
      "history         0.0413\n",
      "word            0.0413\n",
      "association     0.0413\n",
      "translation     0.0401\n",
      "present         0.0401\n",
      "possible        0.0389\n",
      "land            0.0389\n",
      "individual      0.0377\n",
      "general         0.0365\n",
      "abolition       0.0365\n",
      "answer          0.0365\n",
      "principle       0.0365\n",
      "write           0.0354\n",
      "place           0.0354\n",
      "confession      0.0342\n",
      "peasant         0.0342\n",
      "world           0.0342\n",
      "bring           0.0342\n",
      "preface         0.0342\n",
      "abolish         0.0342\n",
      "end             0.0342\n",
      "factory         0.0342\n",
      "faith           0.0342\n",
      "different       0.0330\n",
      "right           0.0330\n",
      "england         0.0318\n",
      "year            0.0318\n",
      "congress        0.0318\n",
      "middle          0.0306\n",
      "measure         0.0306\n",
      "community       0.0306\n",
      "russian         0.0295\n",
      "london          0.0295\n",
      "address         0.0295\n",
      "produce         0.0295\n",
      "point           0.0295\n",
      "early           0.0283\n",
      "shall           0.0283\n",
      "come            0.0283\n",
      "create          0.0271\n",
      "necessary       0.0271\n",
      "result          0.0271\n",
      "character       0.0271\n",
      "second          0.0259\n",
      "course          0.0259\n",
      "day             0.0259\n",
      "change          0.0259\n",
      "follow          0.0259\n",
      "stage           0.0248\n",
      "true            0.0248\n",
      "slave           0.0248\n",
      "use             0.0248\n",
      "idea            0.0248\n",
      "family          0.0248\n",
      "away            0.0248\n",
      "working         0.0248\n",
      "european        0.0248\n",
      "fact            0.0248\n",
      "life            0.0248\n",
      "action          0.0248\n",
      "century         0.0236\n",
      "introduction    0.0236\n",
      "ii              0.0236\n",
      "age             0.0236\n",
      "increase        0.0236\n",
      "need            0.0236\n",
      "machine         0.0224\n",
      "machinery       0.0224\n",
      "public          0.0224\n",
      "army            0.0224\n",
      "progress        0.0224\n",
      "thing           0.0224\n",
      "instrument      0.0224\n",
      "division        0.0224\n",
      "manufacture     0.0224\n",
      "war             0.0224\n",
      "wage            0.0212\n",
      "direct          0.0212\n",
      "small           0.0212\n",
      "live            0.0212\n",
      "trade           0.0212\n",
      "universal       0.0212\n",
      "gradually       0.0212\n",
      "far             0.0212\n",
      "exchange        0.0200\n",
      "hold            0.0200\n",
      "appear          0.0200\n",
      "union           0.0200\n",
      "stand           0.0200\n",
      "mere            0.0200\n",
      "branch          0.0200\n",
      "subsistence     0.0200\n",
      "lose            0.0200\n",
      "education       0.0200\n",
      "labourer        0.0200\n",
      "set             0.0200\n",
      "soon            0.0200\n",
      "america         0.0189\n",
      "proportion      0.0189\n",
      "law             0.0189\n",
      "case            0.0189\n",
      "empire          0.0189\n",
      "freedom         0.0189\n",
      "woman           0.0189\n",
      "break           0.0189\n",
      "destroy         0.0189\n",
      "town            0.0189\n",
      "high            0.0189\n",
      "lay             0.0189\n",
      "view            0.0177\n",
      "good            0.0177\n",
      "support         0.0177\n",
      "republic        0.0177\n",
      "city            0.0177\n",
      "rise            0.0177\n",
      "market          0.0177\n",
      "finally         0.0177\n",
      "mass            0.0177\n",
      "mode            0.0177\n",
      "material        0.0177\n"
     ]
    }
   ],
   "source": [
    "# Performing TF-IDF on the entire text of Wealth of Nations by Adam Smith and Communist Manifesto by Karl Marx\n",
    "#   Aim: to extract intersect of economics terms (among the top 200 TF-IDF of each text)\n",
    "import re\n",
    "import spacy\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.max_length = 5000000 \n",
    "\n",
    "# Reading texts separately\n",
    "with open('A_wealth_of_Nations_Cleaned.txt', 'r', encoding='utf-8') as file:\n",
    "    text1 = file.read()\n",
    "\n",
    "with open('Communist Manifesto.txt', 'r', encoding='utf-8') as file:\n",
    "    text2 = file.read()\n",
    "\n",
    "# Text cleaning + lemmatization function\n",
    "def preprocess(text):\n",
    "    text = text.lower()                                # Convert all characters to lowercase\n",
    "    text = re.sub(r'\\d+', '', text)                    # Remove numbers\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)                # Remove punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()           # Normalize whitespace\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    lemmas = [token.lemma_ for token in doc \n",
    "              if not token.is_stop and not token.is_punct and token.lemma_ != '-PRON-']\n",
    "    return ' '.join(lemmas)\n",
    "\n",
    "# Preprocessing each document\n",
    "cleaned_texts = [preprocess(text1), preprocess(text2)]\n",
    "\n",
    "# TF-IDF vectorization across the two documents\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=1000) # removes stopwords\n",
    "X = vectorizer.fit_transform(cleaned_texts)\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Getting TF-IDF scores for each doc\n",
    "tfidf_scores_doc1 = X.toarray()[0]\n",
    "tfidf_scores_doc2 = X.toarray()[1]\n",
    "\n",
    "# Getting top terms per document\n",
    "def get_top_terms(tfidf_scores, terms, top_n=200):\n",
    "    top_indices = np.argsort(tfidf_scores)[::-1]\n",
    "    return [(terms[i], tfidf_scores[i]) for i in top_indices[:top_n]]\n",
    "\n",
    "top_terms_doc1 = get_top_terms(tfidf_scores_doc1, terms)\n",
    "top_terms_doc2 = get_top_terms(tfidf_scores_doc2, terms)\n",
    "\n",
    "# Displaying results\n",
    "print(\"\\nTop TF-IDF Terms in The Wealth of Nations:\\n\")\n",
    "for word, score in top_terms_doc1:\n",
    "    print(f\"{word:<15} {score:.4f}\")\n",
    "\n",
    "print(\"\\nTop TF-IDF Terms in The Communist Manifesto:\\n\")\n",
    "for word, score in top_terms_doc2:\n",
    "    print(f\"{word:<15} {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276c852a-d499-4614-8a2b-35f0a8a79b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words extracted that are the intersection of the TF-IDF for top 200 words which are solely and unambiguously economic terms:\n",
    "\n",
    "# - Produce\n",
    "# - Trade\n",
    "# - Labour\n",
    "# - Labourer (excluded below as bert uses subword tokenization so this will never be detected and will instead be considered as labour)\n",
    "# - Land\n",
    "# - Capital\n",
    "# - Market\n",
    "# - Manufacture\n",
    "# - Industry\n",
    "# - Work\n",
    "# - Government\n",
    "# - Private\n",
    "# - Wage\n",
    "# - Demand\n",
    "# - Exchange\n",
    "# - Subsistence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f5fc09-523f-4c34-a817-2ec583e434c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#\n",
    "#\n",
    "#\n",
    "#                                   Model: Getting contextual embeddings from BERT for economic terms extracted using TF-IDF\n",
    "#\n",
    "#\n",
    "#\n",
    "# ------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c5c82fa-38cb-47d7-8626-96f7ddab842b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to output_WoN_pairwise_similarity_produce.txt\n",
      "Results saved to output_WoN_pairwise_similarity_trade.txt\n",
      "Results saved to output_WoN_pairwise_similarity_labour.txt\n",
      "Results saved to output_WoN_pairwise_similarity_land.txt\n",
      "Results saved to output_WoN_pairwise_similarity_capital.txt\n",
      "Results saved to output_WoN_pairwise_similarity_market.txt\n",
      "Results saved to output_WoN_pairwise_similarity_manufacture.txt\n",
      "Results saved to output_WoN_pairwise_similarity_industry.txt\n",
      "Results saved to output_WoN_pairwise_similarity_work.txt\n",
      "Results saved to output_WoN_pairwise_similarity_government.txt\n",
      "Results saved to output_WoN_pairwise_similarity_private.txt\n",
      "Results saved to output_WoN_pairwise_similarity_wage.txt\n",
      "Results saved to output_WoN_pairwise_similarity_demand.txt\n",
      "Results saved to output_WoN_pairwise_similarity_exchange.txt\n",
      "Results saved to output_WoN_pairwise_similarity_subsistence.txt\n"
     ]
    }
   ],
   "source": [
    "# Pairwise in-text cosine similarity of BERT contextual embeddings between all instances of each economic term in \n",
    "#   Wealth of Nations\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import json  # Optional: for exporting results\n",
    "\n",
    "# Load model and tokenizer\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "model.eval()\n",
    "\n",
    "# Load text corpus\n",
    "with open('A_wealth_of_Nations_Cleaned.txt', 'r') as file:\n",
    "    corpus = file.read()\n",
    "\n",
    "chunks = corpus.split(\"\\n\\n\")  # Split by paragraphs\n",
    "context_window = 5 # length of context window\n",
    "max_len = 512 # Bert's max token input length\n",
    "stride = 10  # Overlap between windows when chunking\n",
    "\n",
    "target_words = [\n",
    "    \"produce\", \"trade\", \"labour\", \"land\", \"capital\", \"market\", \"manufacture\", \"industry\", \"work\",\n",
    "    \"government\", \"private\", \"wage\", \"demand\", \"exchange\", \"subsistence\"\n",
    "]\n",
    "\n",
    "# Dictionary to store similarities per word\n",
    "similarities_dict_WoN = {}\n",
    "\n",
    "# Function to process a sub-chunk of text and extract embeddings of all instances of the target word\n",
    "def process_text_window(text, chunk_id, subchunk_id, target_word, occurrences):\n",
    "    text = text.lower()  # Normalize to lowercase\n",
    "    inputs = bert_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=max_len)\n",
    "    tokens = bert_tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "\n",
    "    # Disabling gradient calc. \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state[0]\n",
    "\n",
    "    # Finding index of where token matches target word\n",
    "    word_indices = [i for i, token in enumerate(tokens) if token.lower() == target_word.lower()]\n",
    "\n",
    "    # For each instance, storing the context and embedding\n",
    "    for idx in word_indices:\n",
    "        emb = embeddings[idx]\n",
    "        start = max(idx - context_window, 0)\n",
    "        end = min(idx + context_window + 1, len(tokens))\n",
    "        context_tokens = tokens[start:end]\n",
    "        context_text = bert_tokenizer.convert_tokens_to_string(context_tokens)\n",
    "        occurrences.append({\n",
    "            \"chunk_id\": chunk_id,\n",
    "            \"subchunk_id\": subchunk_id,\n",
    "            \"token_idx\": idx,\n",
    "            \"embedding\": emb,\n",
    "            \"context\": context_text\n",
    "        })\n",
    "\n",
    "# Function to process one target word + generate output\n",
    "def process_target_word(target_word):\n",
    "    occurrences = []\n",
    "    similarities = []\n",
    "\n",
    "    # Go through each paragraph chunk\n",
    "    for chunk_id, chunk in enumerate(chunks):\n",
    "        tokens = bert_tokenizer.tokenize(chunk)\n",
    "        if len(tokens) <= max_len:\n",
    "            text = bert_tokenizer.convert_tokens_to_string(tokens)\n",
    "            process_text_window(text, chunk_id, subchunk_id=0, target_word=target_word, occurrences=occurrences)\n",
    "        else:\n",
    "            for i in range(0, len(tokens), max_len - stride):\n",
    "                sub_tokens = tokens[i:i+max_len]\n",
    "                text = bert_tokenizer.convert_tokens_to_string(sub_tokens)\n",
    "                process_text_window(text, chunk_id, subchunk_id=i // (max_len - stride), target_word=target_word, occurrences=occurrences)\n",
    "\n",
    "    if len(occurrences) < 2:\n",
    "        print(f\"Less than two total occurrences of '{target_word}' found.\")\n",
    "        similarities_dict_WoN[target_word] = []\n",
    "        return\n",
    "\n",
    "    output_lines = []\n",
    "    total_similarity = 0.0\n",
    "    count = 0\n",
    "\n",
    "    output_lines.append(f\"Total '{target_word}' occurrences found: {len(occurrences)}\\n\")\n",
    "\n",
    "    # Compute similarities\n",
    "    for i in range(len(occurrences)):\n",
    "        for j in range(i + 1, len(occurrences)):\n",
    "            emb1 = occurrences[i][\"embedding\"]\n",
    "            emb2 = occurrences[j][\"embedding\"]\n",
    "            similarity = F.cosine_similarity(emb1.unsqueeze(0), emb2.unsqueeze(0)).item()\n",
    "            similarities.append(similarity)\n",
    "\n",
    "            total_similarity += similarity\n",
    "            count += 1\n",
    "\n",
    "            output_lines.append(f\"Pair {count}:\")\n",
    "            output_lines.append(f\" - Chunk IDs: {occurrences[i]['chunk_id']} & {occurrences[j]['chunk_id']}\")\n",
    "            output_lines.append(f\" - Subchunk IDs: {occurrences[i]['subchunk_id']} & {occurrences[j]['subchunk_id']}\")\n",
    "            output_lines.append(f\" - Token positions: {occurrences[i]['token_idx']} & {occurrences[j]['token_idx']}\")\n",
    "            output_lines.append(f\" - Context 1: \\\"{occurrences[i]['context']}\\\"\")\n",
    "            output_lines.append(f\" - Context 2: \\\"{occurrences[j]['context']}\\\"\")\n",
    "            output_lines.append(f\" - Cosine Similarity: {similarity:.4f}\\n\")\n",
    "\n",
    "    average_similarity = total_similarity / count\n",
    "    output_lines.append(f\"\\nAverage Cosine Similarity across all pairs for '{target_word}': {average_similarity:.4f}\")\n",
    "\n",
    "    # Save output to file\n",
    "    filename = f\"output_WoN_pairwise_similarity_{target_word}.txt\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(output_lines))\n",
    "\n",
    "    print(f\"Results saved to {filename}\")\n",
    "    similarities_dict_WoN[target_word] = similarities\n",
    "\n",
    "# Process each target word\n",
    "for word in target_words:\n",
    "    process_target_word(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18e784dc-1bb2-4b5c-ac20-c3e020d9d243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict saved to excel\n"
     ]
    }
   ],
   "source": [
    "# Saving the similarities of contextual embeddings for each word to an Excel file\n",
    "import pandas as pd\n",
    "\n",
    "# Function to pad lists in the dictionary so all have equal length ( for df conversion)\n",
    "def pad_dict_values(data):\n",
    "    max_len = max(len(v) for v in data.values())  # Find the longest list\n",
    "    for key in data:\n",
    "        while len(data[key]) < max_len:\n",
    "            data[key].append(None)  # Pad shorter lists with None\n",
    "    return data\n",
    "\n",
    "# Padding the similarity lists to align lengths\n",
    "padded_data = pad_dict_values(similarities_dict_WoN)\n",
    "\n",
    "# Converting the padded dictionary to a DataFrame + export to Excel\n",
    "df = pd.DataFrame(padded_data)\n",
    "df.to_excel(\"similarities_dict_WoN.xlsx\", index=False)\n",
    "\n",
    "print(\"dict saved to excel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dce2168-5fc1-4f30-8c19-6c6e64ccd84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to output_CM_pairwise_similarity_produce.txt\n",
      "Results saved to output_CM_pairwise_similarity_trade.txt\n",
      "Results saved to output_CM_pairwise_similarity_labour.txt\n",
      "Results saved to output_CM_pairwise_similarity_land.txt\n",
      "Results saved to output_CM_pairwise_similarity_capital.txt\n",
      "Results saved to output_CM_pairwise_similarity_market.txt\n",
      "Results saved to output_CM_pairwise_similarity_manufacture.txt\n",
      "Results saved to output_CM_pairwise_similarity_industry.txt\n",
      "Results saved to output_CM_pairwise_similarity_work.txt\n",
      "Results saved to output_CM_pairwise_similarity_government.txt\n",
      "Results saved to output_CM_pairwise_similarity_private.txt\n",
      "Results saved to output_CM_pairwise_similarity_wage.txt\n",
      "Results saved to output_CM_pairwise_similarity_demand.txt\n",
      "Results saved to output_CM_pairwise_similarity_exchange.txt\n",
      "Results saved to output_CM_pairwise_similarity_subsistence.txt\n"
     ]
    }
   ],
   "source": [
    "# Pairwise in-text cosine similarity of embeddings between instances of each economic term in Communist Manifesto\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Load model and tokenizer\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "model.eval()\n",
    "\n",
    "# Load text corpus\n",
    "with open('Communist Manifesto.txt', 'r', encoding='utf-8') as file:\n",
    "    corpus = file.read()\n",
    "\n",
    "chunks = corpus.split(\"\\n\\n\")  # Split by paragraphs\n",
    "context_window = 5 # length of context window\n",
    "max_len = 512 # Bert's max token input length\n",
    "stride = 10  # Overlap between windows when chunking\n",
    "\n",
    "target_words = [\"produce\", \"trade\", \"labour\", \"land\", \"capital\", \"market\", \"manufacture\", \"industry\", \"work\", \"government\", \"private\",\n",
    "                \"wage\", \"demand\", \"exchange\", \"subsistence\"]  # List of words to analyze\n",
    "\n",
    "# Dictionary to store similarities for each word\n",
    "similarities_dict_CM = {}\n",
    "\n",
    "# Function to process a sub-chunk of text and extract embeddings of all instances of the target word\n",
    "def process_text_window(text, chunk_id, subchunk_id, target_word, occurrences):\n",
    "    text = text.lower()  # make text lowercase to prevent case-sensitivity\n",
    "    inputs = bert_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=max_len)\n",
    "    tokens = bert_tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "\n",
    "    # Disabling grad calc.\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state[0]\n",
    "\n",
    "    # Finding instances of where the token matches the target word\n",
    "    word_indices = [i for i, token in enumerate(tokens) if token.lower() == target_word.lower()]\n",
    "    \n",
    "    for idx in word_indices:\n",
    "        emb = embeddings[idx]\n",
    "        start = max(idx - context_window, 0)\n",
    "        end = min(idx + context_window + 1, len(tokens))\n",
    "        context_tokens = tokens[start:end]\n",
    "        context_text = bert_tokenizer.convert_tokens_to_string(context_tokens)\n",
    "        occurrences.append({\n",
    "            \"chunk_id\": chunk_id,\n",
    "            \"subchunk_id\": subchunk_id,\n",
    "            \"token_idx\": idx,\n",
    "            \"embedding\": emb,\n",
    "            \"context\": context_text\n",
    "        })\n",
    "\n",
    "# Function to process one target word + generate output\n",
    "def process_target_word(target_word):\n",
    "    occurrences = []\n",
    "    similarities = []  # List to hold cosine similarities for this word\n",
    "\n",
    "    # Go through each paragraph chunk\n",
    "    for chunk_id, chunk in enumerate(chunks):\n",
    "        tokens = bert_tokenizer.tokenize(chunk)\n",
    "        if len(tokens) <= max_len:\n",
    "            text = bert_tokenizer.convert_tokens_to_string(tokens)\n",
    "            process_text_window(text, chunk_id, subchunk_id=0, target_word=target_word, occurrences=occurrences)\n",
    "        else:\n",
    "            for i in range(0, len(tokens), max_len - stride):\n",
    "                sub_tokens = tokens[i:i+max_len]\n",
    "                text = bert_tokenizer.convert_tokens_to_string(sub_tokens)\n",
    "                process_text_window(text, chunk_id, subchunk_id=i // (max_len - stride), target_word=target_word, occurrences=occurrences)\n",
    "\n",
    "    if len(occurrences) < 2:\n",
    "        print(f\"Less than two total occurrences of '{target_word}' found.\")\n",
    "        similarities_dict_CM[target_word] = []\n",
    "        return\n",
    "\n",
    "    output_lines = []\n",
    "    total_similarity = 0.0\n",
    "    count = 0\n",
    "\n",
    "    output_lines.append(f\"Total '{target_word}' occurrences found: {len(occurrences)}\\n\")\n",
    "\n",
    "    # Compute similarities\n",
    "    for i in range(len(occurrences)):\n",
    "        for j in range(i + 1, len(occurrences)):\n",
    "            emb1 = occurrences[i][\"embedding\"]\n",
    "            emb2 = occurrences[j][\"embedding\"]\n",
    "            similarity = F.cosine_similarity(emb1.unsqueeze(0), emb2.unsqueeze(0)).item()\n",
    "            similarities.append(similarity)\n",
    "\n",
    "            total_similarity += similarity\n",
    "            count += 1\n",
    "\n",
    "            output_lines.append(f\"Pair {count}:\")\n",
    "            output_lines.append(f\" - Chunk IDs: {occurrences[i]['chunk_id']} & {occurrences[j]['chunk_id']}\")\n",
    "            output_lines.append(f\" - Subchunk IDs: {occurrences[i]['subchunk_id']} & {occurrences[j]['subchunk_id']}\")\n",
    "            output_lines.append(f\" - Token positions: {occurrences[i]['token_idx']} & {occurrences[j]['token_idx']}\")\n",
    "            output_lines.append(f\" - Context 1: \\\"{occurrences[i]['context']}\\\"\")\n",
    "            output_lines.append(f\" - Context 2: \\\"{occurrences[j]['context']}\\\"\")\n",
    "            output_lines.append(f\" - Cosine Similarity: {similarity:.4f}\\n\")\n",
    "\n",
    "    average_similarity = total_similarity / count\n",
    "    output_lines.append(f\"\\nAverage Cosine Similarity across all pairs for '{target_word}': {average_similarity:.4f}\")\n",
    "\n",
    "    # Save to file\n",
    "    filename = f\"output_CM_pairwise_similarity_{target_word}.txt\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(output_lines))\n",
    "\n",
    "    print(f\"Results saved to {filename}\")\n",
    "    similarities_dict_CM[target_word] = similarities  # Store similarities for this word\n",
    "\n",
    "# Loop through all target words\n",
    "for word in target_words:\n",
    "    process_target_word(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be950a7e-412e-4523-9570-8b381a8b0aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict saved to excel\n"
     ]
    }
   ],
   "source": [
    "# Saving the similarities of contextual embeddings for each word to an Excel file\n",
    "import pandas as pd\n",
    "\n",
    "# Function to pad all lists (using None items) in the dictionary to the same length\n",
    "def pad_dict_values(data):\n",
    "    max_len = max(len(v) for v in data.values())  # Find the maximum list length\n",
    "    for key in data:\n",
    "        while len(data[key]) < max_len:\n",
    "            data[key].append(None)  # Pad shorter lists with None for alignment\n",
    "    return data\n",
    "\n",
    "# Pad the lists in the dictionary for df conversion\n",
    "padded_data = pad_dict_values(similarities_dict_CM)\n",
    "\n",
    "# Convert the padded dictionary to a df + export it to an Excel file\n",
    "df = pd.DataFrame(padded_data)\n",
    "df.to_excel(\"similarities_dict_CM.xlsx\", index=False)\n",
    "\n",
    "print(\"dict saved to excel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5689d53a-aa3f-459b-a0ac-d9d13b16096d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing word: 'produce'\n",
      "Finished processing 'produce'. Saved to: output_pairwise_cross_produce.txt\n",
      "\n",
      "Processing word: 'trade'\n",
      "Finished processing 'trade'. Saved to: output_pairwise_cross_trade.txt\n",
      "\n",
      "Processing word: 'labour'\n",
      "Finished processing 'labour'. Saved to: output_pairwise_cross_labour.txt\n",
      "\n",
      "Processing word: 'land'\n",
      "Finished processing 'land'. Saved to: output_pairwise_cross_land.txt\n",
      "\n",
      "Processing word: 'capital'\n",
      "Finished processing 'capital'. Saved to: output_pairwise_cross_capital.txt\n",
      "\n",
      "Processing word: 'market'\n",
      "Finished processing 'market'. Saved to: output_pairwise_cross_market.txt\n",
      "\n",
      "Processing word: 'manufacture'\n",
      "Finished processing 'manufacture'. Saved to: output_pairwise_cross_manufacture.txt\n",
      "\n",
      "Processing word: 'industry'\n",
      "Finished processing 'industry'. Saved to: output_pairwise_cross_industry.txt\n",
      "\n",
      "Processing word: 'work'\n",
      "Finished processing 'work'. Saved to: output_pairwise_cross_work.txt\n",
      "\n",
      "Processing word: 'government'\n",
      "Finished processing 'government'. Saved to: output_pairwise_cross_government.txt\n",
      "\n",
      "Processing word: 'private'\n",
      "Finished processing 'private'. Saved to: output_pairwise_cross_private.txt\n",
      "\n",
      "Processing word: 'wage'\n",
      "Finished processing 'wage'. Saved to: output_pairwise_cross_wage.txt\n",
      "\n",
      "Processing word: 'demand'\n",
      "Finished processing 'demand'. Saved to: output_pairwise_cross_demand.txt\n",
      "\n",
      "Processing word: 'exchange'\n",
      "Finished processing 'exchange'. Saved to: output_pairwise_cross_exchange.txt\n",
      "\n",
      "Processing word: 'subsistence'\n",
      "Finished processing 'subsistence'. Saved to: output_pairwise_cross_subsistence.txt\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Load the BERT tokenizer and model (pretrained, lowercase version)\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "model.eval()  # Set the model to evaluation mode so it doesnt compute gradients\n",
    "\n",
    "# Load the two input texts for comparison\n",
    "with open('A_wealth_of_Nations_Cleaned.txt', 'r') as file1, open('Communist Manifesto.txt', 'r', encoding='utf-8') as file2:\n",
    "    text1 = file1.read()\n",
    "    text2 = file2.read()\n",
    "\n",
    "texts = [text1, text2]\n",
    "\n",
    "# List of economic-related words we want to analyze in both texts\n",
    "target_words = [\"produce\", \"trade\", \"labour\", \"land\", \"capital\", \"market\", \"manufacture\", \n",
    "                \"industry\", \"work\", \"government\", \"private\", \"wage\", \"demand\", \"exchange\", \"subsistence\"]\n",
    "\n",
    "# Parameters for processing\n",
    "context_window = 5   # How many tokens to include before and after the target word\n",
    "max_len = 512        # Max number of tokens BERT can handle\n",
    "stride = 10          # Overlap between sliding windows (to not miss words at the boundary)\n",
    "\n",
    "# This will hold the cosine similarity scores for all the target words\n",
    "all_similarities_cross = {}\n",
    "\n",
    "# Goes through the text and extracts all relevant windows that might contain the target word\n",
    "def process_text(text, text_index, target_word, occurrences):\n",
    "    chunks = text.split(\"\\n\\n\")  # Split the text into paragraphs\n",
    "    for chunk_id, chunk in enumerate(chunks):\n",
    "        tokens = bert_tokenizer.tokenize(chunk)\n",
    "        # If the paragraph fits within BERTs limit, process directly\n",
    "        if len(tokens) <= max_len:\n",
    "            input_text = bert_tokenizer.convert_tokens_to_string(tokens)\n",
    "            process_text_window(input_text, text_index, chunk_id, subchunk_id=0, target_word=target_word, occurrences=occurrences)\n",
    "        else:\n",
    "            # If too long, split into overlapping subchunks (sliding window approach)\n",
    "            for i in range(0, len(tokens), max_len - stride):\n",
    "                sub_tokens = tokens[i:i+max_len]\n",
    "                input_text = bert_tokenizer.convert_tokens_to_string(sub_tokens)\n",
    "                process_text_window(input_text, text_index, chunk_id, subchunk_id=i // (max_len - stride), target_word=target_word, occurrences=occurrences)\n",
    "\n",
    "# Looks at one chunk/subchunk, finds the target word, and grabs its embedding + context\n",
    "def process_text_window(text, text_index, chunk_id, subchunk_id, target_word, occurrences):\n",
    "    inputs = bert_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=max_len)\n",
    "    tokens = bert_tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation (saves memory and time)\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state[0]  # Last hidden layer of BERT for each token\n",
    "\n",
    "    # Find all tokens that exactly match the target word\n",
    "    word_indices = [i for i, token in enumerate(tokens) if token.lower() == target_word.lower()]\n",
    "    \n",
    "    for idx in word_indices:\n",
    "        emb = embeddings[idx]\n",
    "        # Grab some context around the word for reporting\n",
    "        start = max(idx - context_window, 0)\n",
    "        end = min(idx + context_window + 1, len(tokens))\n",
    "        context_tokens = tokens[start:end]\n",
    "        context_text = bert_tokenizer.convert_tokens_to_string(context_tokens)\n",
    "        # Save everything we need about this occurrence\n",
    "        occurrences[text_index].append({\n",
    "            \"chunk_id\": chunk_id,\n",
    "            \"subchunk_id\": subchunk_id,\n",
    "            \"token_idx\": idx,\n",
    "            \"embedding\": emb,\n",
    "            \"context\": context_text\n",
    "        })\n",
    "\n",
    "# Basic cosine similarity between two vectors\n",
    "def cosine_similarity(e1, e2):\n",
    "    return F.cosine_similarity(e1.unsqueeze(0), e2.unsqueeze(0)).item()\n",
    "\n",
    "# Loop over each target word and compare embeddings across the two texts\n",
    "for target_word in target_words:\n",
    "    print(f\"\\nProcessing word: '{target_word}'\")\n",
    "    occurrences = [[], []]  # Store occurrences separately for each text\n",
    "    similarities = []       # Similarities for this word\n",
    "\n",
    "    # Run the processing for both texts\n",
    "    process_text(text1, 0, target_word, occurrences)\n",
    "    process_text(text2, 1, target_word, occurrences)\n",
    "\n",
    "    output_filename = f\"output_pairwise_cross_{target_word}.txt\"\n",
    "\n",
    "    # If one of the texts doesn't have the word, skip it\n",
    "    if len(occurrences[0]) == 0 or len(occurrences[1]) == 0:\n",
    "        print(f\"No matching '{target_word}' occurrences in both texts.\")\n",
    "        with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"No matching '{target_word}' occurrences in both texts.\\n\")\n",
    "        all_similarities_cross[target_word] = []\n",
    "    else:\n",
    "        total_similarity = 0.0\n",
    "        count = 0\n",
    "        with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            # Compare every occurrence from text1 with every occurrence from text2\n",
    "            for i, occ1 in enumerate(occurrences[0]):\n",
    "                for j, occ2 in enumerate(occurrences[1]):\n",
    "                    similarity = cosine_similarity(occ1[\"embedding\"], occ2[\"embedding\"])\n",
    "                    similarities.append(similarity)\n",
    "                    total_similarity += similarity\n",
    "                    count += 1\n",
    "\n",
    "                    # Write out the pair info and similarity score\n",
    "                    f.write(f\"\\nPair {count}:\\n\")\n",
    "                    f.write(f\" - Text1 Chunk ID: {occ1['chunk_id']}, Subchunk ID: {occ1['subchunk_id']}\\n\")\n",
    "                    f.write(f\" - Text2 Chunk ID: {occ2['chunk_id']}, Subchunk ID: {occ2['subchunk_id']}\\n\")\n",
    "                    f.write(f\" - Context 1: \\\"{occ1['context']}\\\"\\n\")\n",
    "                    f.write(f\" - Context 2: \\\"{occ2['context']}\\\"\\n\")\n",
    "                    f.write(f\" - Cosine Similarity: {similarity:.4f}\\n\")\n",
    "\n",
    "            # Compute and log the average similarity\n",
    "            average_similarity = total_similarity / count if count > 0 else 0.0\n",
    "            f.write(f\"\\n\\nAverage Cross-Text Cosine Similarity for '{target_word}': {average_similarity:.4f}\\n\")\n",
    "\n",
    "        print(f\"Finished processing '{target_word}'. Saved to: {output_filename}\")\n",
    "        all_similarities_cross[target_word] = similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0cbd4e4-8d16-482c-9097-d86986596a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict saved to excel\n"
     ]
    }
   ],
   "source": [
    "# Saving between-text semantic similarities to an Excel file\n",
    "import pandas as pd\n",
    "\n",
    "# Padding function to make all value lists the same length (so we can create a proper DataFrame)\n",
    "def pad_dict_values(data):\n",
    "    max_len = max(len(v) for v in data.values())  # Find the longest list\n",
    "    for key in data:\n",
    "        while len(data[key]) < max_len:  # Pad shorter lists with None\n",
    "            data[key].append(None)\n",
    "    return data\n",
    "\n",
    "# Pad all the similarity lists so they're the same length\n",
    "padded_data = pad_dict_values(all_similarities_cross)\n",
    "\n",
    "# Turn the dictionary into a DataFrame + save it as an Excel file\n",
    "df = pd.DataFrame(padded_data)\n",
    "df.to_excel(\"all_similarities_cross.xlsx\", index=False)\n",
    "\n",
    "print(\"Dict saved to Excel.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "90c48aa0-8d4e-44d2-9117-f8003755dd13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-Test Results (Within WoN vs. Cross-Corpus):\n",
      "\n",
      "     produce | t-stat: 154.1733 | p-value: 0.00000000\n",
      "       trade | t-stat: 194.6312 | p-value: 0.00000000\n",
      "      labour | t-stat: 150.7831 | p-value: 0.00000000\n",
      "        land | t-stat:  68.2359 | p-value: 0.00000000\n",
      "     capital | t-stat: 222.9732 | p-value: 0.00000000\n",
      "      market | t-stat:  62.4178 | p-value: 0.00000000\n",
      " manufacture | t-stat:  35.1947 | p-value: 0.00000000\n",
      "    industry | t-stat: 145.8185 | p-value: 0.00000000\n",
      "        work | t-stat:  76.8559 | p-value: 0.00000000\n",
      "  government | t-stat:  74.2075 | p-value: 0.00000000\n",
      "     private | t-stat:  43.2182 | p-value: 0.00000000\n",
      "        wage | t-stat:   0.0172 | p-value: 0.98781184\n",
      "      demand | t-stat:  24.6202 | p-value: 0.00000000\n",
      "    exchange | t-stat:  21.8449 | p-value: 0.00000000\n",
      " subsistence | t-stat:   6.2857 | p-value: 0.00000000\n"
     ]
    }
   ],
   "source": [
    "# Performing an independent t-test to compare within-text similarity (WoN) \n",
    "# vs. between-text similarity (WoN vs Communist Manifesto).\n",
    "# Assumes approximately normal distributions, uses Welchs t-test (no equal variance assumption)\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# List of economic terms we're running tests on\n",
    "target_words = [\n",
    "    \"produce\", \"trade\", \"labour\", \"land\", \"capital\", \"market\",\n",
    "    \"manufacture\", \"industry\", \"work\", \"government\", \"private\",\n",
    "    \"wage\", \"demand\", \"exchange\", \"subsistence\"\n",
    "]\n",
    "\n",
    "print(\"T-Test Results (Within WoN vs. Cross-Corpus):\\n\")\n",
    "\n",
    "# Loop through each word to compare distributions\n",
    "for word in target_words:\n",
    "    # Get rid of any None values from the lists (these come from padding or missing entries)\n",
    "    clean_within = [x for x in similarities_dict_WoN.get(word, []) if x is not None]\n",
    "    clean_between = [x for x in all_similarities_cross.get(word, []) if x is not None]\n",
    "\n",
    "    # Only run the t-test if both samples have enough data\n",
    "    if len(clean_within) >= 2 and len(clean_between) >= 2:\n",
    "        # Welchs t-test: safer when variances might not be equal\n",
    "        t_stat, p_value = ttest_ind(clean_within, clean_between, equal_var=False)\n",
    "        print(f\"{word:>12} | t-stat: {t_stat:>8.4f} | p-value: {p_value:.8f}\")\n",
    "    else:\n",
    "        print(f\"{word:>12} | Insufficient data for t-test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0be6f97-e7f9-43c3-8c5d-812833206120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MannWhitney U Test Results (Within WoN vs. Cross-Corpus):\n",
      "\n",
      "     produce | U-stat: 2627204890.00 | p-value: 0.00000000\n",
      "       trade | U-stat: 7561147942.00 | p-value: 0.00000000\n",
      "      labour | U-stat: 32212094685.00 | p-value: 0.00000000\n",
      "        land | U-stat: 3141089493.50 | p-value: 0.00000000\n",
      "     capital | U-stat: 6495454303.00 | p-value: 0.00000000\n",
      "      market | U-stat: 716732611.00 | p-value: 0.00000000\n",
      " manufacture | U-stat: 9707907.00 | p-value: 0.00000000\n",
      "    industry | U-stat: 2362048810.00 | p-value: 0.00000000\n",
      "        work | U-stat: 1224402877.50 | p-value: 0.00000000\n",
      "  government | U-stat: 511484700.00 | p-value: 0.00000000\n",
      "     private | U-stat: 126710196.00 | p-value: 0.00000000\n",
      "        wage | U-stat:    78.00 | p-value: 0.82308591\n",
      "      demand | U-stat: 48983975.50 | p-value: 0.00000000\n",
      "    exchange | U-stat: 44806872.50 | p-value: 0.00000000\n",
      " subsistence | U-stat: 20599472.50 | p-value: 0.00000000\n"
     ]
    }
   ],
   "source": [
    "# Performing a MannWhitney U test to compare within-text similarity (WoN) vs. \n",
    "# between-text similarity (WoN vs Communist Manifesto)\n",
    "# This is a non-parametric testdoesn't assume normal distribution or equal variances\n",
    "import pandas as pd\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "# Load the similarity data for within-text (WoN) and between-text (WoN vs CM)\n",
    "df_within = pd.read_excel(\"similarities_dict_WoN.xlsx\", sheet_name=\"Sheet1\")\n",
    "df_between = pd.read_excel(\"all_similarities_cross.xlsx\", sheet_name=\"Sheet1\")  # Update sheet name if needed\n",
    "\n",
    "# Convert Excel data into dictionaries where each key is a word and value is a list of similarities\n",
    "similarities_dict_WoN = df_within.to_dict(orient=\"list\")\n",
    "all_similarities_cross = df_between.to_dict(orient=\"list\")\n",
    "\n",
    "# Economic terms to analyze\n",
    "target_words = [\n",
    "    \"produce\", \"trade\", \"labour\", \"land\", \"capital\", \"market\",\n",
    "    \"manufacture\", \"industry\", \"work\", \"government\", \"private\",\n",
    "    \"wage\", \"demand\", \"exchange\", \"subsistence\"\n",
    "]\n",
    "\n",
    "print(\"MannWhitney U Test Results (Within WoN vs. Cross-Corpus):\\n\")\n",
    "\n",
    "# Run the U-test for each word\n",
    "for word in target_words:\n",
    "    # Clean out any missing values (from Excel or padding)\n",
    "    clean_within = [x for x in similarities_dict_WoN.get(word, []) if pd.notna(x)]\n",
    "    clean_between = [x for x in all_similarities_cross.get(word, []) if pd.notna(x)]\n",
    "\n",
    "    # Make sure we have enough data to run the test\n",
    "    if len(clean_within) >= 2 and len(clean_between) >= 2:\n",
    "        # MannWhitney U test (two-sided, compares rank distributions)\n",
    "        u_stat, p_value = mannwhitneyu(clean_within, clean_between, alternative='two-sided')\n",
    "        print(f\"{word:>12} | U-stat: {u_stat:>8.2f} | p-value: {p_value:.8f}\")\n",
    "    else:\n",
    "        print(f\"{word:>12} | Insufficient data for U-test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e322d0f1-a1e5-44dd-8dd7-99896d5f2d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Independent t-Test Results (Within CM vs. Cross-Corpus):\n",
      "\n",
      "     produce | t-stat:   3.7899 | p-value: 0.00114451\n",
      "       trade | t-stat:   2.8193 | p-value: 0.00544596\n",
      "      labour | t-stat:   9.7999 | p-value: 0.00000000\n",
      "        land | t-stat:   3.0610 | p-value: 0.00238647\n",
      "     capital | t-stat:  47.1627 | p-value: 0.00000000\n",
      "      market | t-stat:   6.0964 | p-value: 0.00000023\n",
      " manufacture | t-stat:   7.4815 | p-value: 0.00000000\n",
      "    industry | t-stat:  51.6814 | p-value: 0.00000000\n",
      "        work | t-stat:  -5.0778 | p-value: 0.00000048\n",
      "  government | t-stat:  17.4999 | p-value: 0.00000000\n",
      "     private | t-stat:  41.2833 | p-value: 0.00000000\n",
      "        wage | t-stat:   8.0916 | p-value: 0.00000000\n",
      "      demand | t-stat:   1.0613 | p-value: 0.30098189\n",
      "    exchange | t-stat:   6.7336 | p-value: 0.00000000\n",
      " subsistence | t-stat:  11.8584 | p-value: 0.00000000\n"
     ]
    }
   ],
   "source": [
    "# Performing an independent t-test to compare in-text similarities within the Communist Manifesto (CM)\n",
    "# vs. between-text similarities (CM vs Wealth of Nations)\n",
    "# Assumes normal distribution; using Welchs version of t-test since it doesn't assume equal variance\n",
    "import pandas as pd\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Load the similarity data for CM internal pairs and CM vs WoN pairs\n",
    "df_within = pd.read_excel(\"similarities_dict_CM.xlsx\", sheet_name=\"Sheet1\")\n",
    "df_between = pd.read_excel(\"all_similarities_cross.xlsx\", sheet_name=\"Sheet1\")  # Change sheet name if needed\n",
    "\n",
    "# Convert Excel data to dictionaries with structure: {word: [list of similarities]}\n",
    "similarities_dict_CM = df_within.to_dict(orient=\"list\")\n",
    "all_similarities_cross = df_between.to_dict(orient=\"list\")\n",
    "\n",
    "# Economic terms to analyze\n",
    "target_words = [\n",
    "    \"produce\", \"trade\", \"labour\", \"land\", \"capital\", \"market\",\n",
    "    \"manufacture\", \"industry\", \"work\", \"government\", \"private\",\n",
    "    \"wage\", \"demand\", \"exchange\", \"subsistence\"\n",
    "]\n",
    "\n",
    "print(\"Independent t-Test Results (Within CM vs. Cross-Corpus):\\n\")\n",
    "\n",
    "# Run the t-test for each target word\n",
    "for word in target_words:\n",
    "    # Filter out missing (NaN) values\n",
    "    clean_within = [x for x in similarities_dict_CM.get(word, []) if pd.notna(x)]\n",
    "    clean_between = [x for x in all_similarities_cross.get(word, []) if pd.notna(x)]\n",
    "\n",
    "    # Make sure theres enough data to run the test\n",
    "    if len(clean_within) >= 2 and len(clean_between) >= 2:\n",
    "        # Welchs t-test (handles different variances between groups)\n",
    "        t_stat, p_value = ttest_ind(clean_within, clean_between, equal_var=False)\n",
    "        print(f\"{word:>12} | t-stat: {t_stat:>8.4f} | p-value: {p_value:.8f}\")\n",
    "    else:\n",
    "        print(f\"{word:>12} | Insufficient data for t-test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d79014b-123e-4fbd-af38-cf2b838f1a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MannWhitney U Test Results (Within CM vs. Cross-Corpus):\n",
      "\n",
      "     produce | U-stat: 99607.00 | p-value: 0.00066500\n",
      "       trade | U-stat: 1448453.00 | p-value: 0.11762789\n",
      "      labour | U-stat: 99953712.50 | p-value: 0.00000000\n",
      "        land | U-stat: 3531682.50 | p-value: 0.00000063\n",
      "     capital | U-stat: 46578262.50 | p-value: 0.00000000\n",
      "      market | U-stat: 196815.00 | p-value: 0.00000001\n",
      " manufacture | U-stat: 77505.00 | p-value: 0.00000000\n",
      "    industry | U-stat: 108169241.50 | p-value: 0.00000000\n",
      "        work | U-stat: 5389457.00 | p-value: 0.00000014\n",
      "  government | U-stat: 8360798.50 | p-value: 0.00000000\n",
      "     private | U-stat: 13250313.50 | p-value: 0.00000000\n",
      "        wage | U-stat:  8016.00 | p-value: 0.00000000\n",
      "      demand | U-stat: 21880.00 | p-value: 0.53528080\n",
      "    exchange | U-stat: 277183.00 | p-value: 0.00000000\n",
      " subsistence | U-stat: 291989.50 | p-value: 0.00000000\n"
     ]
    }
   ],
   "source": [
    "# Performing a MannWhitney U test to compare in-text similarity within the Communist Manifesto (CM)\n",
    "# vs. between-text similarity (CM vs Wealth of Nations)\n",
    "# This is a non-parametric test, so it doesn't assume normal distribution or equal variancesgood for skewed or noisy data\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "# Load the similarity data from Excel\n",
    "df_within = pd.read_excel(\"similarities_dict_CM.xlsx\", sheet_name=\"Sheet1\")\n",
    "df_between = pd.read_excel(\"all_similarities_cross.xlsx\", sheet_name=\"Sheet1\")  # Update if sheet name is different\n",
    "\n",
    "# Convert DataFrames to dictionaries: {word: [list of similarity scores]}\n",
    "similarities_dict_CM = df_within.to_dict(orient=\"list\")\n",
    "all_similarities_cross = df_between.to_dict(orient=\"list\")\n",
    "\n",
    "# Economic terms we're analyzing\n",
    "target_words = [\n",
    "    \"produce\", \"trade\", \"labour\", \"land\", \"capital\", \"market\",\n",
    "    \"manufacture\", \"industry\", \"work\", \"government\", \"private\",\n",
    "    \"wage\", \"demand\", \"exchange\", \"subsistence\"\n",
    "]\n",
    "\n",
    "print(\"MannWhitney U Test Results (Within CM vs. Cross-Corpus):\\n\")\n",
    "\n",
    "# Loop through each word and run the U-test\n",
    "for word in target_words:\n",
    "    # Remove any missing values (e.g., NaNs from Excel)\n",
    "    clean_within = [x for x in similarities_dict_CM.get(word, []) if pd.notna(x)]\n",
    "    clean_between = [x for x in all_similarities_cross.get(word, []) if pd.notna(x)]\n",
    "\n",
    "    # Only run the test if both groups have enough samples\n",
    "    if len(clean_within) >= 2 and len(clean_between) >= 2:\n",
    "        # Run a two-sided MannWhitney U test (tests for difference in distributions)\n",
    "        u_stat, p_value = mannwhitneyu(clean_within, clean_between, alternative='two-sided')\n",
    "        print(f\"{word:>12} | U-stat: {u_stat:>8.2f} | p-value: {p_value:.8f}\")\n",
    "    else:\n",
    "        print(f\"{word:>12} | Insufficient data for U-test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f45d8f-391a-4c9f-ac72-f95926690043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#\n",
    "#\n",
    "#\n",
    "#                                            Extension: Looking at the penultimate layer of BERT\n",
    "#           Note: the code below is identical to the code above, the only alteration is that the hidden layers are extracted and the last\n",
    "#                 one selected to extract the embeddings from.\n",
    "#\n",
    "#\n",
    "# ------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69520fe1-4d53-46d5-8498-42b676b7d352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to output_WoN_pairwise_similarity_BLLayer_produce.txt\n",
      "Results saved to output_WoN_pairwise_similarity_BLLayer_trade.txt\n",
      "Results saved to output_WoN_pairwise_similarity_BLLayer_labour.txt\n",
      "Results saved to output_WoN_pairwise_similarity_BLLayer_land.txt\n",
      "Results saved to output_WoN_pairwise_similarity_BLLayer_capital.txt\n",
      "Results saved to output_WoN_pairwise_similarity_BLLayer_market.txt\n",
      "Results saved to output_WoN_pairwise_similarity_BLLayer_manufacture.txt\n",
      "Results saved to output_WoN_pairwise_similarity_BLLayer_industry.txt\n",
      "Results saved to output_WoN_pairwise_similarity_BLLayer_work.txt\n",
      "Results saved to output_WoN_pairwise_similarity_BLLayer_government.txt\n",
      "Results saved to output_WoN_pairwise_similarity_BLLayer_private.txt\n",
      "Results saved to output_WoN_pairwise_similarity_BLLayer_wage.txt\n",
      "Results saved to output_WoN_pairwise_similarity_BLLayer_demand.txt\n",
      "Results saved to output_WoN_pairwise_similarity_BLLayer_exchange.txt\n",
      "Results saved to output_WoN_pairwise_similarity_BLLayer_subsistence.txt\n"
     ]
    }
   ],
   "source": [
    "# Pairwise in-text cosine similarity of BERT contextual embeddings between all instances of each economic term in \n",
    "#   Wealth of Nations (Before-last layer)\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import json  # Optional: for exporting results\n",
    "\n",
    "# Load model and tokenizer\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\", output_hidden_states=True)\n",
    "model.eval()\n",
    "\n",
    "# Load text corpus\n",
    "with open('A_wealth_of_Nations_Cleaned.txt', 'r') as file:\n",
    "    corpus = file.read()\n",
    "\n",
    "chunks = corpus.split(\"\\n\\n\")  # Split by paragraphs\n",
    "context_window = 5\n",
    "max_len = 512\n",
    "stride = 10  # Overlap between windows\n",
    "\n",
    "target_words = [\n",
    "    \"produce\", \"trade\", \"labour\", \"land\", \"capital\", \"market\", \"manufacture\", \"industry\", \"work\",\n",
    "    \"government\", \"private\", \"wage\", \"demand\", \"exchange\", \"subsistence\"\n",
    "]\n",
    "\n",
    "# Dictionary to store similarities per word\n",
    "similarities_dict_WoN = {}\n",
    "\n",
    "def process_text_window(text, chunk_id, subchunk_id, target_word, occurrences):\n",
    "    text = text.lower()  # Normalize to lowercase\n",
    "    inputs = bert_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=max_len)\n",
    "    tokens = bert_tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = outputs.hidden_states[-2][0]\n",
    "    \n",
    "    word_indices = [i for i, token in enumerate(tokens) if token.lower() == target_word.lower()]\n",
    "    \n",
    "    for idx in word_indices:\n",
    "        emb = embeddings[idx]\n",
    "        start = max(idx - context_window, 0)\n",
    "        end = min(idx + context_window + 1, len(tokens))\n",
    "        context_tokens = tokens[start:end]\n",
    "        context_text = bert_tokenizer.convert_tokens_to_string(context_tokens)\n",
    "        occurrences.append({\n",
    "            \"chunk_id\": chunk_id,\n",
    "            \"subchunk_id\": subchunk_id,\n",
    "            \"token_idx\": idx,\n",
    "            \"embedding\": emb,\n",
    "            \"context\": context_text\n",
    "        })\n",
    "\n",
    "# Function to process one target word\n",
    "def process_target_word(target_word):\n",
    "    occurrences = []\n",
    "    similarities = []\n",
    "\n",
    "    # Go through each paragraph chunk\n",
    "    for chunk_id, chunk in enumerate(chunks):\n",
    "        tokens = bert_tokenizer.tokenize(chunk)\n",
    "        if len(tokens) <= max_len:\n",
    "            text = bert_tokenizer.convert_tokens_to_string(tokens)\n",
    "            process_text_window(text, chunk_id, subchunk_id=0, target_word=target_word, occurrences=occurrences)\n",
    "        else:\n",
    "            for i in range(0, len(tokens), max_len - stride):\n",
    "                sub_tokens = tokens[i:i+max_len]\n",
    "                text = bert_tokenizer.convert_tokens_to_string(sub_tokens)\n",
    "                process_text_window(text, chunk_id, subchunk_id=i // (max_len - stride), target_word=target_word, occurrences=occurrences)\n",
    "\n",
    "    if len(occurrences) < 2:\n",
    "        print(f\"Less than two total occurrences of '{target_word}' found.\")\n",
    "        similarities_dict_WoN[target_word] = []\n",
    "        return\n",
    "\n",
    "    output_lines = []\n",
    "    total_similarity = 0.0\n",
    "    count = 0\n",
    "\n",
    "    output_lines.append(f\"Total '{target_word}' occurrences found: {len(occurrences)}\\n\")\n",
    "\n",
    "    # Compute similarities\n",
    "    for i in range(len(occurrences)):\n",
    "        for j in range(i + 1, len(occurrences)):\n",
    "            emb1 = occurrences[i][\"embedding\"]\n",
    "            emb2 = occurrences[j][\"embedding\"]\n",
    "            similarity = F.cosine_similarity(emb1.unsqueeze(0), emb2.unsqueeze(0)).item()\n",
    "            similarities.append(similarity)\n",
    "\n",
    "            total_similarity += similarity\n",
    "            count += 1\n",
    "\n",
    "            output_lines.append(f\"Pair {count}:\")\n",
    "            output_lines.append(f\" - Chunk IDs: {occurrences[i]['chunk_id']} & {occurrences[j]['chunk_id']}\")\n",
    "            output_lines.append(f\" - Subchunk IDs: {occurrences[i]['subchunk_id']} & {occurrences[j]['subchunk_id']}\")\n",
    "            output_lines.append(f\" - Token positions: {occurrences[i]['token_idx']} & {occurrences[j]['token_idx']}\")\n",
    "            output_lines.append(f\" - Context 1: \\\"{occurrences[i]['context']}\\\"\")\n",
    "            output_lines.append(f\" - Context 2: \\\"{occurrences[j]['context']}\\\"\")\n",
    "            output_lines.append(f\" - Cosine Similarity: {similarity:.4f}\\n\")\n",
    "\n",
    "    average_similarity = total_similarity / count\n",
    "    output_lines.append(f\"\\nAverage Cosine Similarity across all pairs for '{target_word}': {average_similarity:.4f}\")\n",
    "\n",
    "    # Save output to file\n",
    "    filename = f\"output_WoN_pairwise_similarity_BLLayer_{target_word}.txt\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(output_lines))\n",
    "\n",
    "    print(f\"Results saved to {filename}\")\n",
    "    similarities_dict_WoN[target_word] = similarities\n",
    "\n",
    "# Process each target word\n",
    "for word in target_words:\n",
    "    process_target_word(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ad094f2-0846-4b00-92df-b43e78e79b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict saved to excel\n"
     ]
    }
   ],
   "source": [
    "# Saving the similarities of contextual embeddings for each word to an excel file (Before last BERT layer)\n",
    "import pandas as pd\n",
    "\n",
    "def pad_dict_values(data):\n",
    "    max_len = max(len(v) for v in data.values())\n",
    "    for key in data:\n",
    "        while len(data[key]) < max_len:\n",
    "            data[key].append(None)\n",
    "    return data\n",
    "\n",
    "# Pad the lists\n",
    "padded_data = pad_dict_values(similarities_dict_WoN)\n",
    "\n",
    "# Convert to DataFrame and save to Excel\n",
    "df = pd.DataFrame(padded_data)\n",
    "df.to_excel(\"similarities_dict_WoN_BLLayer.xlsx\", index=False)\n",
    "\n",
    "print(\"dict saved to excel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bcc1cbf7-7227-4005-9fbc-298d78562fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to output_CM_pairwise_similarity_BLLayer_produce.txt\n",
      "Results saved to output_CM_pairwise_similarity_BLLayer_trade.txt\n",
      "Results saved to output_CM_pairwise_similarity_BLLayer_labour.txt\n",
      "Results saved to output_CM_pairwise_similarity_BLLayer_land.txt\n",
      "Results saved to output_CM_pairwise_similarity_BLLayer_capital.txt\n",
      "Results saved to output_CM_pairwise_similarity_BLLayer_market.txt\n",
      "Results saved to output_CM_pairwise_similarity_BLLayer_manufacture.txt\n",
      "Results saved to output_CM_pairwise_similarity_BLLayer_industry.txt\n",
      "Results saved to output_CM_pairwise_similarity_BLLayer_work.txt\n",
      "Results saved to output_CM_pairwise_similarity_BLLayer_government.txt\n",
      "Results saved to output_CM_pairwise_similarity_BLLayer_private.txt\n",
      "Results saved to output_CM_pairwise_similarity_BLLayer_wage.txt\n",
      "Results saved to output_CM_pairwise_similarity_BLLayer_demand.txt\n",
      "Results saved to output_CM_pairwise_similarity_BLLayer_exchange.txt\n",
      "Results saved to output_CM_pairwise_similarity_BLLayer_subsistence.txt\n"
     ]
    }
   ],
   "source": [
    "# Pairwise in-text cosine similarity of embeddings between instances of each economic term in Communist Manifesto \n",
    "#     (Before last BERT layer)\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Load model and tokenizer\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\", output_hidden_states=True)\n",
    "model.eval()\n",
    "\n",
    "# Load text corpus\n",
    "with open('Communist Manifesto.txt', 'r', encoding='utf-8') as file:\n",
    "    corpus = file.read()\n",
    "\n",
    "chunks = corpus.split(\"\\n\\n\")  # Split by paragraphs\n",
    "context_window = 5\n",
    "max_len = 512\n",
    "stride = 10  # Overlap between windows\n",
    "\n",
    "target_words = [\"produce\", \"trade\", \"labour\", \"land\", \"capital\", \"market\", \"manufacture\", \"industry\", \"work\", \"government\", \"private\",\n",
    "                \"wage\", \"demand\", \"exchange\", \"subsistence\"]  # List of words to analyze\n",
    "\n",
    "# Dictionary to store similarities for each word\n",
    "similarities_dict_CM = {}\n",
    "\n",
    "def process_text_window(text, chunk_id, subchunk_id, target_word, occurrences):\n",
    "    text = text.lower()  # make text lowercase to prevent case-sensitivity\n",
    "    inputs = bert_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=max_len)\n",
    "    tokens = bert_tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = outputs.hidden_states[-2][0]\n",
    "    \n",
    "    word_indices = [i for i, token in enumerate(tokens) if token.lower() == target_word.lower()]\n",
    "    \n",
    "    for idx in word_indices:\n",
    "        emb = embeddings[idx]\n",
    "        start = max(idx - context_window, 0)\n",
    "        end = min(idx + context_window + 1, len(tokens))\n",
    "        context_tokens = tokens[start:end]\n",
    "        context_text = bert_tokenizer.convert_tokens_to_string(context_tokens)\n",
    "        occurrences.append({\n",
    "            \"chunk_id\": chunk_id,\n",
    "            \"subchunk_id\": subchunk_id,\n",
    "            \"token_idx\": idx,\n",
    "            \"embedding\": emb,\n",
    "            \"context\": context_text\n",
    "        })\n",
    "\n",
    "# Function to process one target word\n",
    "def process_target_word(target_word):\n",
    "    occurrences = []\n",
    "    similarities = []  # List to hold cosine similarities for this word\n",
    "\n",
    "    # Go through each paragraph chunk\n",
    "    for chunk_id, chunk in enumerate(chunks):\n",
    "        tokens = bert_tokenizer.tokenize(chunk)\n",
    "        if len(tokens) <= max_len:\n",
    "            text = bert_tokenizer.convert_tokens_to_string(tokens)\n",
    "            process_text_window(text, chunk_id, subchunk_id=0, target_word=target_word, occurrences=occurrences)\n",
    "        else:\n",
    "            for i in range(0, len(tokens), max_len - stride):\n",
    "                sub_tokens = tokens[i:i+max_len]\n",
    "                text = bert_tokenizer.convert_tokens_to_string(sub_tokens)\n",
    "                process_text_window(text, chunk_id, subchunk_id=i // (max_len - stride), target_word=target_word, occurrences=occurrences)\n",
    "\n",
    "    if len(occurrences) < 2:\n",
    "        print(f\"Less than two total occurrences of '{target_word}' found.\")\n",
    "        similarities_dict_CM[target_word] = []\n",
    "        return\n",
    "\n",
    "    output_lines = []\n",
    "    total_similarity = 0.0\n",
    "    count = 0\n",
    "\n",
    "    output_lines.append(f\"Total '{target_word}' occurrences found: {len(occurrences)}\\n\")\n",
    "\n",
    "    # Compute similarities\n",
    "    for i in range(len(occurrences)):\n",
    "        for j in range(i + 1, len(occurrences)):\n",
    "            emb1 = occurrences[i][\"embedding\"]\n",
    "            emb2 = occurrences[j][\"embedding\"]\n",
    "            similarity = F.cosine_similarity(emb1.unsqueeze(0), emb2.unsqueeze(0)).item()\n",
    "            similarities.append(similarity)\n",
    "\n",
    "            total_similarity += similarity\n",
    "            count += 1\n",
    "\n",
    "            output_lines.append(f\"Pair {count}:\")\n",
    "            output_lines.append(f\" - Chunk IDs: {occurrences[i]['chunk_id']} & {occurrences[j]['chunk_id']}\")\n",
    "            output_lines.append(f\" - Subchunk IDs: {occurrences[i]['subchunk_id']} & {occurrences[j]['subchunk_id']}\")\n",
    "            output_lines.append(f\" - Token positions: {occurrences[i]['token_idx']} & {occurrences[j]['token_idx']}\")\n",
    "            output_lines.append(f\" - Context 1: \\\"{occurrences[i]['context']}\\\"\")\n",
    "            output_lines.append(f\" - Context 2: \\\"{occurrences[j]['context']}\\\"\")\n",
    "            output_lines.append(f\" - Cosine Similarity: {similarity:.4f}\\n\")\n",
    "\n",
    "    average_similarity = total_similarity / count\n",
    "    output_lines.append(f\"\\nAverage Cosine Similarity across all pairs for '{target_word}': {average_similarity:.4f}\")\n",
    "\n",
    "    # Save to file\n",
    "    filename = f\"output_CM_pairwise_similarity_BLLayer_{target_word}.txt\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(output_lines))\n",
    "\n",
    "    print(f\"Results saved to {filename}\")\n",
    "    similarities_dict_CM[target_word] = similarities  # Store similarities for this word\n",
    "\n",
    "# Loop through all target words\n",
    "for word in target_words:\n",
    "    process_target_word(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98dcfd78-bb7d-490a-a4ce-01fc94e42ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict saved to excel\n"
     ]
    }
   ],
   "source": [
    "# Saving the similarities of contextual embeddings for each word to an excel file (Before last layer)\n",
    "import pandas as pd\n",
    "\n",
    "def pad_dict_values(data):\n",
    "    max_len = max(len(v) for v in data.values())\n",
    "    for key in data:\n",
    "        while len(data[key]) < max_len:\n",
    "            data[key].append(None)\n",
    "    return data\n",
    "\n",
    "# Pad the lists\n",
    "padded_data = pad_dict_values(similarities_dict_CM)\n",
    "\n",
    "# Convert to DataFrame and save to Excel\n",
    "df = pd.DataFrame(padded_data)\n",
    "df.to_excel(\"similarities_dict_CM_BLLayer.xlsx\", index=False)\n",
    "\n",
    "print(\"dict saved to excel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44e8c01d-4c52-42ae-97a1-7b76c754c202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing word: 'produce'\n",
      "Finished processing 'produce'. Saved to: output_pairwise_cross_BLLayer_produce.txt\n",
      "\n",
      "Processing word: 'trade'\n",
      "Finished processing 'trade'. Saved to: output_pairwise_cross_BLLayer_trade.txt\n",
      "\n",
      "Processing word: 'labour'\n",
      "Finished processing 'labour'. Saved to: output_pairwise_cross_BLLayer_labour.txt\n",
      "\n",
      "Processing word: 'land'\n",
      "Finished processing 'land'. Saved to: output_pairwise_cross_BLLayer_land.txt\n",
      "\n",
      "Processing word: 'capital'\n",
      "Finished processing 'capital'. Saved to: output_pairwise_cross_BLLayer_capital.txt\n",
      "\n",
      "Processing word: 'market'\n",
      "Finished processing 'market'. Saved to: output_pairwise_cross_BLLayer_market.txt\n",
      "\n",
      "Processing word: 'manufacture'\n",
      "Finished processing 'manufacture'. Saved to: output_pairwise_cross_BLLayer_manufacture.txt\n",
      "\n",
      "Processing word: 'industry'\n",
      "Finished processing 'industry'. Saved to: output_pairwise_cross_BLLayer_industry.txt\n",
      "\n",
      "Processing word: 'work'\n",
      "Finished processing 'work'. Saved to: output_pairwise_cross_BLLayer_work.txt\n",
      "\n",
      "Processing word: 'government'\n",
      "Finished processing 'government'. Saved to: output_pairwise_cross_BLLayer_government.txt\n",
      "\n",
      "Processing word: 'private'\n",
      "Finished processing 'private'. Saved to: output_pairwise_cross_BLLayer_private.txt\n",
      "\n",
      "Processing word: 'wage'\n",
      "Finished processing 'wage'. Saved to: output_pairwise_cross_BLLayer_wage.txt\n",
      "\n",
      "Processing word: 'demand'\n",
      "Finished processing 'demand'. Saved to: output_pairwise_cross_BLLayer_demand.txt\n",
      "\n",
      "Processing word: 'exchange'\n",
      "Finished processing 'exchange'. Saved to: output_pairwise_cross_BLLayer_exchange.txt\n",
      "\n",
      "Processing word: 'subsistence'\n",
      "Finished processing 'subsistence'. Saved to: output_pairwise_cross_BLLayer_subsistence.txt\n"
     ]
    }
   ],
   "source": [
    "# Pairwise between-text cosine similarity of embeddings between instances of each economic term in both texts (Before last layer)\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Load model and tokenizer\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\", output_hidden_states=True)\n",
    "model.eval()\n",
    "\n",
    "# Load two separate texts\n",
    "with open('A_wealth_of_Nations_Cleaned.txt', 'r') as file1, open('Communist Manifesto.txt', 'r', encoding='utf-8') as file2:\n",
    "    text1 = file1.read()\n",
    "    text2 = file2.read()\n",
    "\n",
    "texts = [text1, text2]\n",
    "target_words = [\"produce\", \"trade\", \"labour\", \"land\", \"capital\", \"market\", \"manufacture\", \"industry\", \"work\", \"government\", \"private\",\n",
    "                \"wage\", \"demand\", \"exchange\", \"subsistence\"]\n",
    "context_window = 5\n",
    "max_len = 512\n",
    "stride = 10\n",
    "\n",
    "# Dictionary to store all cosine similarities for each word\n",
    "all_similarities_cross = {}\n",
    "\n",
    "def process_text(text, text_index, target_word, occurrences):\n",
    "    chunks = text.split(\"\\n\\n\")\n",
    "    for chunk_id, chunk in enumerate(chunks):\n",
    "        tokens = bert_tokenizer.tokenize(chunk)\n",
    "        if len(tokens) <= max_len:\n",
    "            input_text = bert_tokenizer.convert_tokens_to_string(tokens)\n",
    "            process_text_window(input_text, text_index, chunk_id, subchunk_id=0, target_word=target_word, occurrences=occurrences)\n",
    "        else:\n",
    "            for i in range(0, len(tokens), max_len - stride):\n",
    "                sub_tokens = tokens[i:i+max_len]\n",
    "                input_text = bert_tokenizer.convert_tokens_to_string(sub_tokens)\n",
    "                process_text_window(input_text, text_index, chunk_id, subchunk_id=i // (max_len - stride), target_word=target_word, occurrences=occurrences)\n",
    "\n",
    "def process_text_window(text, text_index, chunk_id, subchunk_id, target_word, occurrences):\n",
    "    inputs = bert_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=max_len)\n",
    "    tokens = bert_tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = outputs.hidden_states[-2][0]\n",
    "\n",
    "    word_indices = [i for i, token in enumerate(tokens) if token.lower() == target_word.lower()]\n",
    "    \n",
    "    for idx in word_indices:\n",
    "        emb = embeddings[idx]\n",
    "        start = max(idx - context_window, 0)\n",
    "        end = min(idx + context_window + 1, len(tokens))\n",
    "        context_tokens = tokens[start:end]\n",
    "        context_text = bert_tokenizer.convert_tokens_to_string(context_tokens)\n",
    "        occurrences[text_index].append({\n",
    "            \"chunk_id\": chunk_id,\n",
    "            \"subchunk_id\": subchunk_id,\n",
    "            \"token_idx\": idx,\n",
    "            \"embedding\": emb,\n",
    "            \"context\": context_text\n",
    "        })\n",
    "\n",
    "def cosine_similarity(e1, e2):\n",
    "    return F.cosine_similarity(e1.unsqueeze(0), e2.unsqueeze(0)).item()\n",
    "\n",
    "# Loop through each target word\n",
    "for target_word in target_words:\n",
    "    print(f\"\\nProcessing word: '{target_word}'\")\n",
    "    occurrences = [[], []]  # Reset for each word\n",
    "    similarities = []  # Store similarities for current word\n",
    "\n",
    "    process_text(text1, 0, target_word, occurrences)\n",
    "    process_text(text2, 1, target_word, occurrences)\n",
    "\n",
    "    output_filename = f\"output_pairwise_cross_BLLayer_{target_word}.txt\"\n",
    "\n",
    "    if len(occurrences[0]) == 0 or len(occurrences[1]) == 0:\n",
    "        print(f\"No matching '{target_word}' occurrences in both texts.\")\n",
    "        with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"No matching '{target_word}' occurrences in both texts.\\n\")\n",
    "        all_similarities_cross[target_word] = []\n",
    "    else:\n",
    "        total_similarity = 0.0\n",
    "        count = 0\n",
    "        with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            for i, occ1 in enumerate(occurrences[0]):\n",
    "                for j, occ2 in enumerate(occurrences[1]):\n",
    "                    similarity = cosine_similarity(occ1[\"embedding\"], occ2[\"embedding\"])\n",
    "                    similarities.append(similarity)\n",
    "                    total_similarity += similarity\n",
    "                    count += 1\n",
    "\n",
    "                    f.write(f\"\\nPair {count}:\\n\")\n",
    "                    f.write(f\" - Text1 Chunk ID: {occ1['chunk_id']}, Subchunk ID: {occ1['subchunk_id']}\\n\")\n",
    "                    f.write(f\" - Text2 Chunk ID: {occ2['chunk_id']}, Subchunk ID: {occ2['subchunk_id']}\\n\")\n",
    "                    f.write(f\" - Context 1: \\\"{occ1['context']}\\\"\\n\")\n",
    "                    f.write(f\" - Context 2: \\\"{occ2['context']}\\\"\\n\")\n",
    "                    f.write(f\" - Cosine Similarity: {similarity:.4f}\\n\")\n",
    "            \n",
    "            average_similarity = total_similarity / count if count > 0 else 0.0\n",
    "            f.write(f\"\\n\\nAverage Cross-Text Cosine Similarity for '{target_word}': {average_similarity:.4f}\\n\")\n",
    "\n",
    "        print(f\"Finished processing '{target_word}'. Saved to: {output_filename}\")\n",
    "        all_similarities_cross[target_word] = similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f1be5b5-06a6-42f2-bba6-d789ba7dd875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict saved to excel\n"
     ]
    }
   ],
   "source": [
    "# Saving between-text semantic similarities to an excel file (Before last layer)\n",
    "import pandas as pd\n",
    "\n",
    "def pad_dict_values(data):\n",
    "    max_len = max(len(v) for v in data.values())\n",
    "    for key in data:\n",
    "        while len(data[key]) < max_len:\n",
    "            data[key].append(None)\n",
    "    return data\n",
    "\n",
    "# Pad the lists\n",
    "padded_data = pad_dict_values(all_similarities_cross)\n",
    "\n",
    "# Convert to DataFrame and save to Excel\n",
    "df = pd.DataFrame(padded_data)\n",
    "df.to_excel(\"all_similarities_cross_BLLayer.xlsx\", index=False)\n",
    "\n",
    "print(\"dict saved to excel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b317efe-2bbf-4dc3-b644-adea6758648b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-Test Results (Within WoN vs. Cross-Corpus):\n",
      "\n",
      "     produce | t-stat: 132.9392 | p-value: 0.00000000\n",
      "       trade | t-stat: 142.4930 | p-value: 0.00000000\n",
      "      labour | t-stat: 111.8835 | p-value: 0.00000000\n",
      "        land | t-stat:  43.0238 | p-value: 0.00000000\n",
      "     capital | t-stat: 117.5366 | p-value: 0.00000000\n",
      "      market | t-stat:  33.7599 | p-value: 0.00000000\n",
      " manufacture | t-stat:  24.1471 | p-value: 0.00000000\n",
      "    industry | t-stat:  97.6770 | p-value: 0.00000000\n",
      "        work | t-stat:  56.5666 | p-value: 0.00000000\n",
      "  government | t-stat:  61.1348 | p-value: 0.00000000\n",
      "     private | t-stat:  44.7714 | p-value: 0.00000000\n",
      "        wage | t-stat:   0.0155 | p-value: 0.98903855\n",
      "      demand | t-stat:  17.1447 | p-value: 0.00000000\n",
      "    exchange | t-stat:   3.8294 | p-value: 0.00013001\n",
      " subsistence | t-stat:   3.7976 | p-value: 0.00014811\n"
     ]
    }
   ],
   "source": [
    "# Performing an Independent t-test (assumes normality and equal variances) of WoN in-text vs Between-text (with CM):\n",
    "#   (Before last layer)\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# List of target words to analyze\n",
    "target_words = [\n",
    "    \"produce\", \"trade\", \"labour\", \"land\", \"capital\", \"market\",\n",
    "    \"manufacture\", \"industry\", \"work\", \"government\", \"private\",\n",
    "    \"wage\", \"demand\", \"exchange\", \"subsistence\"\n",
    "]\n",
    "\n",
    "print(\"T-Test Results (Within WoN vs. Cross-Corpus):\\n\")\n",
    "\n",
    "for word in target_words:\n",
    "    # Clean the similarity lists of None items\n",
    "    clean_within = [x for x in similarities_dict_WoN.get(word, []) if x is not None]\n",
    "    clean_between = [x for x in all_similarities_cross.get(word, []) if x is not None]\n",
    "\n",
    "    if len(clean_within) >= 2 and len(clean_between) >= 2:\n",
    "        t_stat, p_value = ttest_ind(clean_within, clean_between, equal_var=False)  # Welchs t-test\n",
    "        print(f\"{word:>12} | t-stat: {t_stat:>8.4f} | p-value: {p_value:.8f}\")\n",
    "    else:\n",
    "        print(f\"{word:>12} | Insufficient data for t-test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "20b01868-bbde-4b3b-a3e7-372775d07cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MannWhitney U Test Results (Within WoN vs. Cross-Corpus):\n",
      "\n",
      "     produce | U-stat: 2562424121.00 | p-value: 0.00000000\n",
      "       trade | U-stat: 6888655491.00 | p-value: 0.00000000\n",
      "      labour | U-stat: 29889434032.00 | p-value: 0.00000000\n",
      "        land | U-stat: 2846635268.50 | p-value: 0.00000000\n",
      "     capital | U-stat: 5459039826.50 | p-value: 0.00000000\n",
      "      market | U-stat: 608345930.00 | p-value: 0.00000000\n",
      " manufacture | U-stat: 8667709.50 | p-value: 0.00000000\n",
      "    industry | U-stat: 2103772711.00 | p-value: 0.00000000\n",
      "        work | U-stat: 1131213585.50 | p-value: 0.00000000\n",
      "  government | U-stat: 485735819.50 | p-value: 0.00000000\n",
      "     private | U-stat: 127041938.50 | p-value: 0.00000000\n",
      "        wage | U-stat:    93.00 | p-value: 0.82308591\n",
      "      demand | U-stat: 45438448.50 | p-value: 0.00000000\n",
      "    exchange | U-stat: 36900535.50 | p-value: 0.21272192\n",
      " subsistence | U-stat: 19370140.50 | p-value: 0.00000105\n"
     ]
    }
   ],
   "source": [
    "# Performing a MannWhitney U test (non-parametric) of WoN in-text vs Between-text (with CM):\n",
    "#   (Before last layer)\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "# List of target words to analyze\n",
    "target_words = [\n",
    "    \"produce\", \"trade\", \"labour\", \"land\", \"capital\", \"market\",\n",
    "    \"manufacture\", \"industry\", \"work\", \"government\", \"private\",\n",
    "    \"wage\", \"demand\", \"exchange\", \"subsistence\"\n",
    "]\n",
    "\n",
    "print(\"MannWhitney U Test Results (Within WoN vs. Cross-Corpus):\\n\")\n",
    "\n",
    "for word in target_words:\n",
    "    # Clean the similarity lists of None items\n",
    "    clean_within = [x for x in similarities_dict_WoN.get(word, []) if x is not None]\n",
    "    clean_between = [x for x in all_similarities_cross.get(word, []) if x is not None]\n",
    "\n",
    "    if len(clean_within) >= 2 and len(clean_between) >= 2:\n",
    "        u_stat, p_value = mannwhitneyu(clean_within, clean_between, alternative='two-sided')\n",
    "        print(f\"{word:>12} | U-stat: {u_stat:>8.2f} | p-value: {p_value:.8f}\")\n",
    "    else:\n",
    "        print(f\"{word:>12} | Insufficient data for U-test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f3de9ee0-f9cf-4f35-9947-c4bb1fbb5368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-Test Results (Within CM vs. Cross-Corpus):\n",
      "\n",
      "     produce | t-stat:   4.4446 | p-value: 0.00024689\n",
      "       trade | t-stat:   4.2951 | p-value: 0.00003078\n",
      "      labour | t-stat:   9.7747 | p-value: 0.00000000\n",
      "        land | t-stat:   2.9383 | p-value: 0.00353179\n",
      "     capital | t-stat:  45.6676 | p-value: 0.00000000\n",
      "      market | t-stat:   6.4501 | p-value: 0.00000007\n",
      " manufacture | t-stat:   7.0934 | p-value: 0.00000000\n",
      "    industry | t-stat:  42.1114 | p-value: 0.00000000\n",
      "        work | t-stat:  -3.7112 | p-value: 0.00022136\n",
      "  government | t-stat:  13.0559 | p-value: 0.00000000\n",
      "     private | t-stat:  34.3947 | p-value: 0.00000000\n",
      "        wage | t-stat:   6.2555 | p-value: 0.00000003\n",
      "      demand | t-stat:   2.0151 | p-value: 0.05721642\n",
      "    exchange | t-stat:   6.7124 | p-value: 0.00000000\n",
      " subsistence | t-stat:  11.3956 | p-value: 0.00000000\n"
     ]
    }
   ],
   "source": [
    "# Performing an Independent t-test (assumes normality and equal variances) of CM in-text vs Between-text (with WoN):\n",
    "#   (Before last layer)\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# List of target words to analyze\n",
    "target_words = [\n",
    "    \"produce\", \"trade\", \"labour\", \"land\", \"capital\", \"market\",\n",
    "    \"manufacture\", \"industry\", \"work\", \"government\", \"private\",\n",
    "    \"wage\", \"demand\", \"exchange\", \"subsistence\"\n",
    "]\n",
    "\n",
    "print(\"T-Test Results (Within CM vs. Cross-Corpus):\\n\")\n",
    "\n",
    "for word in target_words:\n",
    "    # Clean the similarity lists of None items\n",
    "    clean_within = [x for x in similarities_dict_CM.get(word, []) if x is not None]\n",
    "    clean_between = [x for x in all_similarities_cross.get(word, []) if x is not None]\n",
    "\n",
    "    if len(clean_within) >= 2 and len(clean_between) >= 2:\n",
    "        t_stat, p_value = ttest_ind(clean_within, clean_between, equal_var=False)  # Welchs t-test\n",
    "        print(f\"{word:>12} | t-stat: {t_stat:>8.4f} | p-value: {p_value:.8f}\")\n",
    "    else:\n",
    "        print(f\"{word:>12} | Insufficient data for t-test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d52561d5-0f27-4a04-adc1-e617c28751d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MannWhitney U Test Results (Within CM vs. Cross-Corpus):\n",
      "\n",
      "     produce | U-stat: 104932.00 | p-value: 0.00006092\n",
      "       trade | U-stat: 1542919.00 | p-value: 0.00222772\n",
      "      labour | U-stat: 99281825.00 | p-value: 0.00000000\n",
      "        land | U-stat: 3532333.50 | p-value: 0.00000060\n",
      "     capital | U-stat: 45494096.50 | p-value: 0.00000000\n",
      "      market | U-stat: 197960.00 | p-value: 0.00000001\n",
      " manufacture | U-stat: 74725.00 | p-value: 0.00000000\n",
      "    industry | U-stat: 102298326.50 | p-value: 0.00000000\n",
      "        work | U-stat: 5557546.00 | p-value: 0.00005545\n",
      "  government | U-stat: 7894068.50 | p-value: 0.00000000\n",
      "     private | U-stat: 12703990.00 | p-value: 0.00000000\n",
      "        wage | U-stat:  7201.00 | p-value: 0.00000007\n",
      "      demand | U-stat: 23923.00 | p-value: 0.15712363\n",
      "    exchange | U-stat: 278702.00 | p-value: 0.00000000\n",
      " subsistence | U-stat: 287709.50 | p-value: 0.00000000\n"
     ]
    }
   ],
   "source": [
    "# Performing a MannWhitney U test (non-parametric) of CM in-text vs Between-text (with WoN):\n",
    "#   (Before last layer)\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "# List of target words to analyze\n",
    "target_words = [\n",
    "    \"produce\", \"trade\", \"labour\", \"land\", \"capital\", \"market\",\n",
    "    \"manufacture\", \"industry\", \"work\", \"government\", \"private\",\n",
    "    \"wage\", \"demand\", \"exchange\", \"subsistence\"\n",
    "]\n",
    "\n",
    "print(\"MannWhitney U Test Results (Within CM vs. Cross-Corpus):\\n\")\n",
    "\n",
    "for word in target_words:\n",
    "    # Clean the similarity lists of None items\n",
    "    clean_within = [x for x in similarities_dict_CM.get(word, []) if x is not None]\n",
    "    clean_between = [x for x in all_similarities_cross.get(word, []) if x is not None]\n",
    "\n",
    "    if len(clean_within) >= 2 and len(clean_between) >= 2:\n",
    "        u_stat, p_value = mannwhitneyu(clean_within, clean_between, alternative='two-sided')\n",
    "        print(f\"{word:>12} | U-stat: {u_stat:>8.2f} | p-value: {p_value:.8f}\")\n",
    "    else:\n",
    "        print(f\"{word:>12} | Insufficient data for U-test\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
